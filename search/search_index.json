{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Home","text":"etl4s <p>Powerful, whiteboard-style ETL.</p> Get Started Try Online GitHub ChainConfigDiagramTelemetry <pre><code>import etl4s._\n\n/* Define building blocks */\nval fiveExtract = Extract(5)\nval timesTwo    = Transform[Int, Int](_ * 2)\nval plusFive    = Transform[Int, Int](_ + 5)\nval exclaim     = Transform[Int, String](x =&gt; s\"Result: $x!\")\nval consoleLoad = Load[String, Unit](println)\nval dbLoad      = Load[String, Unit](x =&gt; println(s\"[DB] $x\"))\n\n/* Compose with `andThen` */\nval timesTwoPlusFive = timesTwo `andThen` plusFive\n\n/* Stitch with ~&gt; */\nval pipeline = fiveExtract ~&gt; timesTwoPlusFive ~&gt; exclaim ~&gt; (consoleLoad &amp; dbLoad)\n\n/* Run */\npipeline.unsafeRun()\n// Result: 15!\n// [DB] Result: 15!\n</code></pre> <pre><code>import etl4s._\n\ncase class Env(path: String)\n\nval load = Load[String, Unit].requires[Env] { env =&gt; data =&gt;\n  println(s\"Writing to ${env.path}\")\n}\n\nval pipeline = extract ~&gt; transform ~&gt; load\n\npipeline.provide(Env(\"s3://dev\")).unsafeRun()\npipeline.provide(Env(\"s3://prod\")).unsafeRun()\n</code></pre> <pre><code>import etl4s._\n\nval A = Node[String, String](identity)\n  .lineage(name = \"A\", inputs = List(\"s1\", \"s2\"), outputs = List(\"s3\"))\n\nval B = Node[String, String](identity)\n  .lineage(name = \"B\", inputs = List(\"s3\"), outputs = List(\"s4\", \"s5\"))\n\nSeq(A, B).toMermaid\n</code></pre> <pre><code>graph LR\n    classDef pipeline fill:#e1f5fe,stroke:#01579b,stroke-width:2px,color:#000\n    classDef dataSource fill:#f3e5f5,stroke:#4a148c,stroke-width:2px,color:#000\n\n    A[\"A\"]\n    B[\"B\"]\n    s1([\"s1\"])\n    s2([\"s2\"])\n    s3([\"s3\"])\n    s4([\"s4\"])\n    s5([\"s5\"])\n\n    s1 --&gt; A\n    s2 --&gt; A\n    A --&gt; s3\n    s3 --&gt; B\n    B --&gt; s4\n    B --&gt; s5\n\n    class A pipeline\n    class B pipeline\n    class s1,s2,s3,s4,s5 dataSource</code></pre> <pre><code>import etl4s._\n\nval process = Transform[List[Row], List[Row]] { rows =&gt;\n  Tel.addCounter(\"rows.processed\", rows.size)\n  Tel.setGauge(\"batch.size\", rows.size.toDouble)\n  rows.filter(_.isValid)\n}\n\nprocess.unsafeRun(rows)  // no-ops (zero cost)\n\nimplicit val t: Etl4sTelemetry = Prometheus()\nprocess.unsafeRun(rows)  // metrics flowing\n</code></pre> How it works 1 Import Drop one file into your project. No dependencies, no framework lock-in. 2 Chain Connect nodes with <code>~&gt;</code>, branch with <code>&amp;</code>, inject config with <code>.requires</code> 3 Run Call <code>.unsafeRun()</code>. Works in scripts, Spark, Flink, anywhere Scala runs. Pipelines as values. <p>One file, zero dependencies. Lazy, composable, testable. Since pipelines are values, attach metadata, generate lineage diagrams, share them across teams.</p> // One import. That's it. import etl4s._ val pipeline =   extract ~&gt; transform ~&gt; load Type-safe composition. <p>Types must align or it won't compile. Misconnections are compile errors.</p> E[A, Int] ~&gt; T[Int, Str] ~&gt; L[Str, B] won't compile Dependency injection, inferred. <p>Nodes declare what they need. Chain freely. The compiler merges and infers the combined type.</p> Needs[Db] ~&gt; Needs[Api] = Needs[Db &amp; Api] Built-in tracing. <p>Shared execution state across pipeline nodes. Write logs, flag errors, react to upstream failures, track timing. Retrieve with <code>.unsafeRunTrace()</code>.</p> E log ~&gt; T check log ~&gt; L check Trace logs \"read 1420 rows\" \"validated 89\" time 0 1 2 3 4 ms \u2192 Trace[B] result: B logs: 2 errors: 1 time: 4ms Runs anywhere. <p>JVM, JavaScript, WebAssembly, native binaries via LLVM. Same code, zero platform-specific APIs.</p> WA LLVM Why etl4s? <p>Chaotic, framework-coupled ETL codebases drive dev teams to their knees. etl4s lets you structure your code as clean, typed graphs of pure functions.</p> <p>(~&gt;) is just *chef's kiss*. There are so many synergies here, haven't pushed for something this hard in a while.Sr Engineering Manager, Instacart</p> <p>...the advantages of full blown effect systems without the complexities, and awkward monad syntax!u/RiceBroad4552</p> <p>Battle-tested at Instacart \ud83e\udd55</p> Installation First Pipeline Core Concepts Examples"},{"location":"advanced/","title":"Advanced","text":""},{"location":"advanced/#reusable-components","title":"Reusable Components","text":"<p>Group parameterized transforms into domain modules:</p> <pre><code>object CustomerOps {\n\n  def activeOnly =\n    Transform[List[Customer], List[Customer]](_.filter(_.isActive))\n\n  def topSpenders(n: Int) =\n    Transform[List[Customer], List[Customer]](_.sortBy(-_.spend).take(n))\n\n  def inRegion(region: String) =\n    Transform[List[Customer], List[Customer]](_.filter(_.region == region))\n}\n\nimport CustomerOps._\nval pipeline = extract ~&gt; activeOnly ~&gt; inRegion(\"EU\") ~&gt; topSpenders(100) ~&gt; load\n</code></pre>"},{"location":"advanced/#custom-operators","title":"Custom Operators","text":"<p>Add domain-specific operators via extension methods:</p> <pre><code>extension [A, B](node: Node[A, B]) {\n  def timed(label: String): Node[A, B] = Node { input =&gt;\n    val start = System.currentTimeMillis()\n    val result = node(input)\n    Trace.log(s\"$label: ${System.currentTimeMillis() - start}ms\")\n    result\n  }\n}\n\nval pipeline = extract ~&gt; transform.timed(\"main\") ~&gt; load\n</code></pre>"},{"location":"advanced/#symbolic-operators","title":"Symbolic Operators","text":"<p>Define your own:</p> <pre><code>extension [A, B](node: Node[A, B]) {\n  def !!(attempts: Int): Node[A, B] = node.withRetry(attempts)\n  def @@(label: String): Node[A, B] = node.tap(_ =&gt; Trace.log(label))\n}\n\nval pipeline = extract ~&gt; riskyTransform !! 3 ~&gt; load @@ \"done\"\n</code></pre>"},{"location":"branching/","title":"Conditional Branching","text":"<p>Route data through different pipelines using <code>If</code>, <code>ElseIf</code>, and <code>Else</code>. Branch on the data itself or on configuration.</p> <pre><code>import etl4s._\n\ncase class JobConfig(isBackfill: Boolean)\n\nval pipeline = extract ~&gt; validate\n  .If(cfg =&gt; _ =&gt; cfg.isBackfill) (fullLoad ~&gt; dedupe ~&gt; save)\n  .Else                           (deltaLoad ~&gt; save)\n</code></pre> <p>Branch on data:</p> <pre><code>val classify = Node[Int, Int](identity)\n  .If(_ &gt; 0)     (Node(_ =&gt; \"positive\"))\n  .ElseIf(_ &lt; 0) (Node(_ =&gt; \"negative\"))\n  .Else          (Node(_ =&gt; \"zero\"))\n\nclassify.unsafeRun(5)   // \"positive\"\nclassify.unsafeRun(-3)  // \"negative\"\nclassify.unsafeRun(0)   // \"zero\"\n</code></pre> <p>The condition is checked against the input, and only the matching branch executes.</p>"},{"location":"branching/#composing-pipelines-in-branches","title":"Composing Pipelines in Branches","text":"<p>Each branch can be a full pipeline, not just a single node:</p> <pre><code>val router = Node[User, User](identity)\n  .If(_.tier == \"premium\")      (validate ~&gt; enrich ~&gt; toPremiumResult)\n  .ElseIf(_.tier == \"standard\") (validate ~&gt; toStandardResult)\n  .Else                         (toGuestResult)\n</code></pre>"},{"location":"branching/#combining-with-fan-out","title":"Combining with Fan-out","text":"<p>Branches can include parallel operations using <code>&amp;</code> or <code>&amp;&gt;</code>:</p> <pre><code>val router = Node[User, User](identity)\n  .If(_.wantsDetails) ((identity &amp; loadMetrics &amp; loadHistory) ~&gt; toFullProfile)\n  .Else               (toSimpleProfile)\n</code></pre>"},{"location":"branching/#config-aware-branching","title":"Config-Aware Branching","text":"<p>When your pipeline uses configuration, conditions can access it too:</p> <pre><code>case class Config(threshold: Int)\n\nval router = Node[Config, Int, Int](identity)\n  .If(cfg =&gt; n =&gt; n &gt;= cfg.threshold) (premiumPath)\n  .Else                               (standardPath)\n\nrouter.provide(Config(100)).unsafeRun(150)  // premium\nrouter.provide(Config(100)).unsafeRun(50)   // standard\n</code></pre>"},{"location":"branching/#scala-2-vs-scala-3","title":"Scala 2 vs Scala 3","text":"<p>The API is identical across versions, but Scala 3's type system enables more flexibility.</p> <p>Scala 3 - branches can return different types (union):</p> <pre><code>val router = Node[Int, Int](identity)\n  .If(_ &gt; 0)     (Node(n =&gt; s\"pos-$n\"))    // String\n  .ElseIf(_ &lt; 0) (Node(n =&gt; n * -1))       // Int\n  .Else          (Node(n =&gt; n.toDouble))   // Double\n// Result type: String | Int | Double\n</code></pre> <p>Scala 3 - branches can require different configs (intersection):</p> <pre><code>val router = Node[String, String](identity)\n  .If(cfg =&gt; _.startsWith(\"db:\")) (dbBranch)    // needs DbConfig\n  .Else                           (cacheBranch) // needs CacheConfig\n// Must provide: DbConfig &amp; CacheConfig\n</code></pre> <p>Scala 2</p> <p>All branches must return the same type: <pre><code>val router = Node[Int, Int](identity)\n  .If(_ &gt; 0)     (Node(n =&gt; s\"pos-$n\"))\n  .ElseIf(_ &lt; 0) (Node(n =&gt; s\"neg-$n\"))\n  .Else          (Node(_ =&gt; \"zero\"))\n// All branches return String\n</code></pre></p>"},{"location":"config/","title":"Configuration","text":"<p>When writing pipelines, you often need to:</p> <ul> <li>Pass database URLs, API keys, thresholds to various stages</li> <li>Avoid threading config through every function signature</li> <li>Keep stages testable by swapping config at the edge</li> </ul> <p><code>.requires</code> declares what a node needs. <code>.provide</code> supplies it once at the top. Config flows through automatically - it's just a Reader monad (<code>Config =&gt; Node[In, Out]</code>) with some syntax.</p> <pre><code>import etl4s._\n\ncase class Cfg(key: String)\n\nval A = Extract(\"data\")\nval B = Transform[String, String].requires[Cfg] { cfg =&gt; data =&gt;\n  s\"${cfg.key}: $data\"\n}\n\nval pipeline = A ~&gt; B\n\npipeline.provide(Cfg(\"secret\")).unsafeRun(())  // \"secret: data\"\n</code></pre>"},{"location":"config/#config-propagation","title":"Config Propagation","text":"<p>Build modular configs with traits. etl4s infers what your pipeline needs:</p> <pre><code>trait HasDb { def dbUrl: String }\ntrait HasAuth { def apiKey: String }\n\nval A = Load[String, Unit].requires[HasDb] { cfg =&gt; data =&gt;\n  println(s\"Saving to ${cfg.dbUrl}: $data\")\n}\n\nval B = Extract[Unit, String].requires[HasAuth] { cfg =&gt; _ =&gt;\n  s\"Fetched with ${cfg.apiKey}\"\n}\n\nval C = Transform[String, String](_.toUpperCase)\n\ncase class AppConfig(dbUrl: String, apiKey: String) extends HasDb with HasAuth\n\nval pipeline = B ~&gt; C ~&gt; A\n\npipeline.provide(AppConfig(\"jdbc:pg\", \"secret-key\")).unsafeRun(())\n</code></pre>"},{"location":"config/#context","title":"Context","text":"<p><code>Context[T]</code> organizes config-driven nodes into modules:</p> <pre><code>case class DbConfig(url: String, timeout: Int)\n\nobject DataPipeline extends Context[DbConfig] {\n\n  val fetch = Context.Extract[Unit, String] { cfg =&gt; _ =&gt;\n    s\"Connected to ${cfg.url} with timeout ${cfg.timeout}s\"\n  }\n\n  val save = Context.Load[String, Unit] { cfg =&gt; data =&gt;\n    println(s\"Saving to ${cfg.url}: $data\")\n  }\n\n  val pipeline = fetch ~&gt; save\n}\n\nDataPipeline.pipeline.provide(DbConfig(\"jdbc:pg\", 5000)).unsafeRun(())\n</code></pre> <p>Scala 2</p> <p>Use explicit types for better inference: <pre><code>Transform.requires[Config, String, String] { cfg =&gt; input =&gt;\n  cfg.key + input\n}\n</code></pre> In Scala 3, the preferred syntax is: <pre><code>Transform[String, String].requires[Config] { cfg =&gt; input =&gt;\n  cfg.key + input\n}\n</code></pre></p>"},{"location":"core-concepts/","title":"Core Concepts","text":"<p>etl4s has one core building block: <pre><code>Node[-In, +Out]\n</code></pre> A Node wraps a lazily-evaluated function <code>In =&gt; Out</code>. Chain them with <code>~&gt;</code> to build pipelines.</p>"},{"location":"core-concepts/#node-types","title":"Node types","text":"<p>To improve readability and express intent, etl4s defines four aliases: <code>Extract</code>, <code>Transform</code>, <code>Load</code> and <code>Pipeline</code>. All behave the same under the hood.</p> <pre><code>type Extract[-In, +Out]   = Node[In, Out]\ntype Transform[-In, +Out] = Node[In, Out]\ntype Load[-In, +Out]      = Node[In, Out]\ntype Pipeline[-In, +Out]  = Node[In, Out]\n</code></pre>"},{"location":"core-concepts/#building-pipelines","title":"Building pipelines","text":"<pre><code>import etl4s._\n\nval A = Extract(\"users.csv\")\nval B = Transform[String, Int](csv =&gt; csv.split(\"\\n\").length)\nval C = Load[Int, Unit](count =&gt; println(s\"Processed $count users\"))\n\nval pipeline = A ~&gt; B ~&gt; C\n\npipeline.unsafeRun()// Processed 3 users\n</code></pre> <p>Create standalone nodes: <pre><code>val toUpper = Transform[String, String](_.toUpperCase)\ntoUpper(\"hello\")  // HELLO\n</code></pre></p>"},{"location":"core-concepts/#running-pipelines","title":"Running pipelines","text":"<p>Call like a function: <pre><code>pipeline(())\n</code></pre></p> <p>Or be explicit: <pre><code>pipeline.unsafeRun()\n</code></pre></p> <p>Error handling: <pre><code>val risky = Pipeline[String, Int](_.toInt)\n\nrisky.safeRun(\"42\")    // Success(42)\nrisky.safeRun(\"oops\")  // Failure(...)\n</code></pre></p> <p>Execution details: <pre><code>val trace = pipeline.unsafeRunTrace(())\n// trace.result, trace.logs, trace.timeElapsedMillis, trace.errors\n\nval safeTrace = pipeline.safeRunTrace(())\n// safeTrace.result is a Try[Out]\n</code></pre></p> <p>Note</p> <p>etl4s also has a <code>Reader</code> type for dependency injection. Use <code>.requires</code> to turn any Node into a <code>Reader[Config, Node]</code>. The <code>~&gt;</code> operator works between Nodes and Readers. See Configuration for details.</p>"},{"location":"effect/","title":"Side Effects","text":""},{"location":"effect/#observing-with-tap","title":"Observing with <code>.tap()</code>","text":"<p>Peek at values mid-pipeline without modifying them:</p> <pre><code>import etl4s._\n\nval pipeline = Extract(\"hello world\")\n  .tap(x =&gt; println(s\"Got: $x\"))\n  ~&gt; Transform[String, Array[String]](_.split(\" \"))\n\npipeline.unsafeRun(())\n// prints: Got: hello world\n// returns: Array(\"hello\", \"world\")\n</code></pre> <p>Chain taps at different stages:</p> <pre><code>val pipeline = fetchData\n  .tap(files =&gt; println(s\"Fetched ${files.size} files\"))\n  ~&gt; processFiles\n  .tap(count =&gt; println(s\"Processed $count files\"))\n</code></pre>"},{"location":"effect/#sequential-effects-with","title":"Sequential Effects with <code>&gt;&gt;</code>","text":"<p>Run multiple nodes in order, same input to each. Only the last result is kept:</p> <pre><code>val logStart = Node[String, Unit](s =&gt; println(s\"Start: $s\"))\nval logEnd   = Node[String, Unit](s =&gt; println(s\"End: $s\"))\nval process  = Node[String, Int](_.length)\n\nval pipeline = logStart &gt;&gt; logEnd &gt;&gt; process\n\npipeline.unsafeRun(\"hello\")\n// prints: Start: hello\n// prints: End: hello\n// returns: 5\n</code></pre> <p>Common for setup/teardown:</p> <pre><code>val clearCache = Node { println(\"Clearing cache...\") }\nval warmCache  = Node { println(\"Warming cache...\") }\n\nval pipeline = clearCache &gt;&gt; warmCache &gt;&gt; mainPipeline\n</code></pre> <p>Or audit logging:</p> <pre><code>val auditStart = Node[Request, Unit](r =&gt; log(s\"Started ${r.id}\"))\nval auditEnd   = Node[Request, Unit](r =&gt; log(s\"Finished ${r.id}\"))\n\nval pipeline = auditStart &gt;&gt; processRequest &gt;&gt; auditEnd\n</code></pre>"},{"location":"effect/#laziness","title":"Laziness","text":"<p>Side effects are lazy - nothing executes until <code>.unsafeRun()</code>. Build and compose freely without triggering effects.</p>"},{"location":"examples-flink/","title":"etl4s + Flink","text":"<p>etl4s structures your Flink job logic. Define extraction, transformation, and sinks as composable, type-safe stages.</p> <pre><code>scala-cli repl --dep io.github.mattlianje::etl4s:1.9.0 --dep org.apache.flink::flink-streaming-scala:1.18.0\n</code></pre>"},{"location":"examples-flink/#basic-streaming-pattern","title":"Basic streaming pattern","text":"<pre><code>import etl4s._\nimport org.apache.flink.streaming.api.scala._\nimport org.apache.flink.streaming.api.scala.StreamExecutionEnvironment\n\nval env = StreamExecutionEnvironment.getExecutionEnvironment\n\ncase class Event(id: String, value: Int, timestamp: Long)\n\nval extractEvents = Extract[StreamExecutionEnvironment, DataStream[Event]] { env =&gt;\n  env.addSource(new KafkaSource[Event](...))\n}\n\nval filterValid = Transform[DataStream[Event], DataStream[Event]] { stream =&gt;\n  stream.filter(_.value &gt; 0)\n}\n\nval aggregate = Transform[DataStream[Event], DataStream[(String, Int)]] { stream =&gt;\n  stream\n    .keyBy(_.id)\n    .timeWindow(Time.minutes(5))\n    .sum(\"value\")\n    .map(e =&gt; (e.id, e.value))\n}\n\nval sinkResults = Load[DataStream[(String, Int)], Unit] { stream =&gt;\n  stream.addSink(new FlinkKafkaProducer(...))\n}\n\nval pipeline =\n  extractEvents ~&gt;\n  filterValid ~&gt;\n  aggregate ~&gt;\n  sinkResults\n\npipeline.unsafeRun(env)\nenv.execute(\"etl4s-flink-job\")\n</code></pre>"},{"location":"examples-flink/#with-config","title":"With config","text":"<pre><code>case class FlinkConfig(\n  kafkaBootstrap: String,\n  inputTopic: String,\n  outputTopic: String,\n  windowMinutes: Int\n)\n\nval extract = Extract[StreamExecutionEnvironment, DataStream[Event]]\n  .requires[FlinkConfig] { config =&gt; env =&gt;\n    val props = new Properties()\n    props.setProperty(\"bootstrap.servers\", config.kafkaBootstrap)\n\n    env.addSource(\n      new FlinkKafkaConsumer(config.inputTopic, new EventSchema(), props)\n    )\n  }\n\nval transform = Transform[DataStream[Event], DataStream[Event]]\n  .requires[FlinkConfig] { config =&gt; stream =&gt;\n    stream\n      .keyBy(_.id)\n      .timeWindow(Time.minutes(config.windowMinutes))\n      .reduce((e1, e2) =&gt; e1.copy(value = e1.value + e2.value))\n  }\n\nval sink = Load[DataStream[Event], Unit]\n  .requires[FlinkConfig] { config =&gt; stream =&gt;\n    stream.addSink(\n      new FlinkKafkaProducer(config.outputTopic, new EventSchema(), ...)\n    )\n  }\n\nval pipeline = extract ~&gt; transform ~&gt; sink\n\nval config = FlinkConfig(\n  kafkaBootstrap = \"localhost:9092\",\n  inputTopic = \"events\",\n  outputTopic = \"results\",\n  windowMinutes = 5\n)\n\npipeline.provide(config).unsafeRun(env)\nenv.execute()\n</code></pre>"},{"location":"examples-flink/#multiple-streams","title":"Multiple streams","text":"<pre><code>val extractUsers = Extract[StreamExecutionEnvironment, DataStream[User]](...)\nval extractEvents = Extract[StreamExecutionEnvironment, DataStream[Event]](...)\n\nval join = Transform[(DataStream[User], DataStream[Event]), DataStream[Enriched]] {\n  case (users, events) =&gt;\n    events\n      .connect(users.broadcast())\n      .process(new JoinFunction())\n}\n\nval pipeline = (extractUsers &amp; extractEvents) ~&gt; join ~&gt; sink\n\npipeline.unsafeRun(env)\nenv.execute()\n</code></pre> <p>Note</p> <p>Use <code>&amp;</code> not <code>&amp;&gt;</code> with Flink - Flink handles parallelism internally. For many streams, use a Map instead of chaining <code>&amp;</code>: <pre><code>val streams = Map(\n  \"users\" -&gt; env.addSource(userSource),\n  \"events\" -&gt; env.addSource(eventSource),\n  \"metrics\" -&gt; env.addSource(metricsSource)\n)\nval extract = Extract(streams)\n</code></pre></p>"},{"location":"examples-spark/","title":"etl4s + Spark","text":"<p>etl4s works alongside Spark. Use it to structure your Spark job logic - extraction, transformations, and loading stay composable and type-safe.</p> <pre><code>scala-cli repl --dep io.github.mattlianje::etl4s:1.9.0 --dep org.apache.spark::spark-sql:3.5.0\n</code></pre>"},{"location":"examples-spark/#basic-pattern","title":"Basic pattern","text":"<pre><code>import etl4s._\nimport org.apache.spark.sql.{SparkSession, DataFrame}\n\nimplicit val spark: SparkSession = SparkSession.builder()\n  .appName(\"etl4s-spark\")\n  .getOrCreate()\n\nval extractUsers = Extract[SparkSession, DataFrame] { spark =&gt;\n  spark.read.parquet(\"s3://data/users\")\n}\n\nval filterActive = Transform[DataFrame, DataFrame] { df =&gt;\n  df.filter($\"active\" === true)\n}\n\nval aggregateByRegion = Transform[DataFrame, DataFrame] { df =&gt;\n  df.groupBy($\"region\").count()\n}\n\nval writeResults = Load[DataFrame, Unit] { df =&gt;\n  df.write.mode(\"overwrite\").parquet(\"s3://output/results\")\n}\n\nval pipeline =\n  extractUsers ~&gt;\n  filterActive ~&gt;\n  aggregateByRegion ~&gt;\n  writeResults\n\npipeline.unsafeRun(spark)\n</code></pre>"},{"location":"examples-spark/#with-config-injection","title":"With config injection","text":"<pre><code>case class SparkConfig(\n  inputPath: String,\n  outputPath: String,\n  partitions: Int\n)\n\nval extract = Extract[SparkSession, DataFrame]\n  .requires[SparkConfig] { config =&gt; spark =&gt;\n    spark.read.parquet(config.inputPath)\n  }\n\nval transform = Transform[DataFrame, DataFrame]\n  .requires[SparkConfig] { config =&gt; df =&gt;\n    df.repartition(config.partitions)\n      .filter($\"valid\" === true)\n  }\n\nval load = Load[DataFrame, Unit]\n  .requires[SparkConfig] { config =&gt; df =&gt;\n    df.write.mode(\"overwrite\").parquet(config.outputPath)\n  }\n\nval pipeline = extract ~&gt; transform ~&gt; load\n\nval config = SparkConfig(\n  inputPath = \"s3://data/raw\",\n  outputPath = \"s3://data/processed\",\n  partitions = 200\n)\n\npipeline.provide(config).unsafeRun(spark)\n</code></pre>"},{"location":"examples-spark/#multiple-data-sources","title":"Multiple data sources","text":"<pre><code>val extractUsers = Extract[SparkSession, DataFrame](\n  _.read.parquet(\"s3://data/users\")\n)\n\nval extractOrders = Extract[SparkSession, DataFrame](\n  _.read.parquet(\"s3://data/orders\")\n)\n\nval join = Transform[(DataFrame, DataFrame), DataFrame] { case (users, orders) =&gt;\n  users.join(orders, users(\"id\") === orders(\"user_id\"))\n}\n\nval pipeline = (extractUsers &amp; extractOrders) ~&gt; join ~&gt; writeResults\n\npipeline.unsafeRun(spark)\n</code></pre> <p>Note</p> <p>Use <code>&amp;</code> not <code>&amp;&gt;</code> with Spark - Spark handles parallelism internally. For many sources, use a Map instead of chaining <code>&amp;</code>: <pre><code>val sources = Map(\n  \"users\" -&gt; spark.read.parquet(\"s3://users\"),\n  \"orders\" -&gt; spark.read.parquet(\"s3://orders\"),\n  \"products\" -&gt; spark.read.parquet(\"s3://products\")\n)\nval extract = Extract(sources)\n</code></pre></p>"},{"location":"examples/","title":"Common Patterns","text":""},{"location":"examples/#chain-pipelines","title":"Chain pipelines","text":"<pre><code>import etl4s._\n\nval A = Pipeline((i: Int) =&gt; i.toString)\nval B = Pipeline((s: String) =&gt; s + \"!\")\n\nval C = A ~&gt; B  // Int =&gt; String\n</code></pre>"},{"location":"examples/#parallel-extraction","title":"Parallel extraction","text":"<pre><code>val e1 = Extract(1)\nval e2 = Extract(\"two\")\nval e3 = Extract(3.0)\n\nval combined = e1 &amp; e2 &amp; e3  // (Int, String, Double)\n</code></pre>"},{"location":"examples/#debugging-with-tap","title":"Debugging with <code>.tap</code>","text":"<p>Inspect values mid-pipeline without affecting the flow:</p> <pre><code>val pipeline = extract\n  .tap(data =&gt; println(s\"Extracted: $data\"))\n  ~&gt; transform\n  .tap(result =&gt; println(s\"Transformed: $result\"))\n  ~&gt; load\n</code></pre>"},{"location":"examples/#sequential-side-effects-with","title":"Sequential side-effects with <code>&gt;&gt;</code>","text":"<p>Run multiple effects in order, same input to each. Only the last result is returned:</p> <pre><code>val logStart  = Node[String, Unit](s =&gt; println(s\"Starting: $s\"))\nval logMiddle = Node[String, Unit](s =&gt; println(s\"Processing: $s\"))\nval process   = Node[String, Int](_.length)\n\nval pipeline = logStart &gt;&gt; logMiddle &gt;&gt; process\n\npipeline.unsafeRun(\"hello\")\n// prints: Starting: hello\n// prints: Processing: hello\n// returns: 5\n</code></pre> <p>Useful for setup/teardown:</p> <pre><code>val clearCache = Node { println(\"Clearing cache...\") }\nval warmCache  = Node { println(\"Warming cache...\") }\n\nval pipeline = clearCache &gt;&gt; warmCache &gt;&gt; mainPipeline\n</code></pre>"},{"location":"examples/#conditional-branching","title":"Conditional branching","text":"<p>Route data based on conditions:</p> <pre><code>val classify = Node[Int, Int](identity)\n  .If(_ &lt; 0)(Node(_ =&gt; \"negative\"))\n  .ElseIf(_ == 0)(Node(_ =&gt; \"zero\"))\n  .Else(Node(_ =&gt; \"positive\"))\n\nclassify.unsafeRun(-5)  // \"negative\"\nclassify.unsafeRun(0)   // \"zero\"\nclassify.unsafeRun(10)  // \"positive\"\n</code></pre>"},{"location":"examples/#error-handling-with-onfailure","title":"Error handling with <code>.onFailure</code>","text":"<p>Provide fallback values:</p> <pre><code>val risky = Node[String, Int](_.toInt)\n  .onFailure(_ =&gt; -1)\n\nrisky.unsafeRun(\"42\")   // 42\nrisky.unsafeRun(\"bad\")  // -1\n</code></pre>"},{"location":"examples/#retry-with-backoff","title":"Retry with backoff","text":"<pre><code>val flaky = Node[String, Response](callExternalApi)\n  .withRetry(maxAttempts = 3, initialDelayMs = 100, backoffMultiplier = 2.0)\n</code></pre>"},{"location":"examples/#reactive-pipelines-with-trace","title":"Reactive pipelines with Trace","text":"<p>Branch on upstream errors:</p> <pre><code>val upstream = Transform[String, Int] { input =&gt;\n  if (input.isEmpty) Trace.error(\"Empty input\")\n  input.length\n}\n\nval downstream = Transform[Int, String] { value =&gt;\n  if (Trace.hasErrors) \"FALLBACK\"\n  else s\"Length: $value\"\n}\n\nval pipeline = upstream ~&gt; downstream\n\npipeline.unsafeRun(\"\")      // \"FALLBACK\"\npipeline.unsafeRun(\"hello\") // \"Length: 5\"\n</code></pre>"},{"location":"failures/","title":"Error Handling","text":"<p>etl4s provides built-in failure handling:</p>"},{"location":"failures/#withretry","title":".withRetry","text":"<p>Retry failed operations with exponential backoff using <code>.withRetry</code>: <pre><code>import etl4s._\n\nvar attempts = 0\n\nval riskyTransformWithRetry = Transform[Int, String] {\n    n =&gt;\n      attempts += 1\n      if (attempts &lt; 3) throw new RuntimeException(s\"Attempt $attempts failed\")\n      else s\"Success after $attempts attempts\"\n}.withRetry(maxAttempts = 3, initialDelayMs = 10)\n\nval pipeline = Extract(42) ~&gt; riskyTransformWithRetry\npipeline.unsafeRun(())\n</code></pre> Output: <pre><code>Success after 3 attempts\n</code></pre></p>"},{"location":"failures/#onfailure","title":".onFailure","text":"<p>Catch exceptions and provide fallback values using <code>.onFailure</code>: <pre><code>import etl4s._\n\nval riskyExtract =\n    Extract[Unit, String](_ =&gt; throw new RuntimeException(\"Boom!\"))\n\nval safeExtract = riskyExtract.onFailure(e =&gt; s\"Failed: ${e.getMessage}\")\nval consoleLoad = Load[String, Unit](println(_))\n\nval pipeline = safeExtract ~&gt; consoleLoad\npipeline.unsafeRun(())\n</code></pre> Output: <pre><code>Failed: Boom!\n</code></pre></p>"},{"location":"faq/","title":"FAQ","text":""},{"location":"faq/#general","title":"General","text":"<p>Q: What is etl4s? A single-file, zero-dependency Scala library for expressing code as composable pipelines. Chain with <code>~&gt;</code>, parallelize with <code>&amp;</code>, inject dependencies with <code>.requires</code>.</p> <p>Q: Is this a framework? No, and never will be. It's an ultralight library that doesn't impose a worldview. Try it zero-cost on one pipeline today.</p> <p>Q: Does this replace Spark/Flink/Pandas? No. etl4s structures your pipeline logic. You still use Spark/Flink/Pandas for actual data processing. etl4s makes that code composable and type-safe.</p> <p>Q: Is this a workflow orchestrator like Airflow? No. etl4s doesn't schedule jobs or manage distributed execution. Use Airflow or any scheduler for that. etl4s structures the code those tools run.</p> <p>Q: Where can I use it? Anywhere: local scripts, web servers, alongside any framework like Spark or Flink.</p> <p>Q: Can I use this in production? Yes. It powers grocery deliveries at Instacart. Type safety catches bugs at compile time. No runtime dependencies means nothing to break.</p>"},{"location":"faq/#how-it-works","title":"How it works","text":"<p>Q: What does <code>~&gt;</code> actually do? Connects pipeline stages. It's an overloaded symbolic operator that works with plain nodes (<code>Node[In, Out]</code>) or nodes that need config (<code>Reader[Env, Node[In, Out]]</code>). Mix them freely - the operator figures out what environment is needed. If two stages need different configs, it automatically merges them.</p>"},{"location":"faq/#usage","title":"Usage","text":"<p>Q: What happens if a stage fails? Execution halts immediately. Use <code>.safeRun()</code> to get a <code>Try[Result]</code>, or handle errors with <code>.onFailure()</code>.</p> <p>Q: Can I mix sync and async code? Yes. All stages run as effects. You can have blocking and non-blocking operations in the same pipeline.</p>"},{"location":"faq/#observability","title":"Observability","text":"<p>Q: How do I know what happened during execution? Call <code>.unsafeRunTrace()</code> instead of <code>.unsafeRun()</code>. Returns <code>Trace</code> with logs, errors, and timing:</p> <pre><code>val trace = pipeline.unsafeRunTrace(data)\ntrace.logs                // everything logged during execution\ntrace.errors              // all errors encountered\ntrace.timeElapsedMillis   // how long it took\n</code></pre> <p>Q: How does tracing work? Uses ThreadLocal to collect logs and errors during execution. Any stage can call <code>Trace.log()</code> or <code>Trace.error()</code>. Downstream stages see upstream issues automatically via <code>Trace.current</code> - no passing state through function parameters.</p> <p>Q: How do I add metrics? Use <code>Tel.addCounter()</code>, <code>Tel.setGauge()</code>, <code>Tel.recordHistogram()</code> in your stages. Zero-cost by default. Provide <code>Etl4sTelemetry</code> implementation to light them up in prod:</p> <pre><code>val process = Transform[List[User], Int] { users =&gt;\n  Tel.addCounter(\"users_processed\", users.size)\n  users.filter(_.isValid).length\n}\n</code></pre> <p>Q: Can I use this with Prometheus/DataDog/etc? Yes. Implement the <code>Etl4sTelemetry</code> trait for your backend. See the Telemetry docs.</p>"},{"location":"first-pipeline/","title":"Your First Pipeline","text":"<p>Let's build a pipeline in 5 minutes.</p>"},{"location":"first-pipeline/#a-node-wraps-a-function","title":"A Node wraps a function","text":"<pre><code>import etl4s._\n\nval double = Transform[Int, Int](_ * 2)\n\ndouble(5)  // 10\n</code></pre> <p>That's it. A <code>Node</code> wraps a function. You can call it like a function.</p>"},{"location":"first-pipeline/#chain-with","title":"Chain with <code>~&gt;</code>","text":"<pre><code>val double = Transform[Int, Int](_ * 2)\nval addTen = Transform[Int, Int](_ + 10)\n\nval pipeline = double ~&gt; addTen\n\npipeline(5)  // 20\n</code></pre> <p>The <code>~&gt;</code> operator chains nodes. Output of left becomes input of right. Types must match or it won't compile.</p>"},{"location":"first-pipeline/#extract-transform-load","title":"Extract, Transform, Load","text":"<pre><code>val extract = Extract(5)                           // starts with 5\nval transform = Transform[Int, Int](_ * 2)         // double it\nval load = Load[Int, Unit](x =&gt; println(s\"Result: $x\"))  // print it\n\nval pipeline = extract ~&gt; transform ~&gt; load\n\npipeline.unsafeRun(())  // prints \"Result: 10\"\n</code></pre> <p><code>Extract</code>, <code>Transform</code>, <code>Load</code> are just aliases for <code>Node</code>. Use them to show intent.</p>"},{"location":"first-pipeline/#run-in-parallel-with","title":"Run in parallel with <code>&amp;</code>","text":"<pre><code>val double = Transform[Int, Int](_ * 2)\nval triple = Transform[Int, Int](_ * 3)\n\nval combine = Transform[(Int, Int), Int] { case (a, b) =&gt; a + b }\n\nval pipeline = Extract(5) ~&gt; (double &amp; triple) ~&gt; combine\n\npipeline.unsafeRun(())  // (10, 15) -&gt; 25\n</code></pre> <p><code>(double &amp; triple)</code> runs both in parallel. Results collected as a tuple and passed to <code>combine</code>.</p>"},{"location":"first-pipeline/#add-config-with-requires","title":"Add config with <code>.requires</code>","text":"<pre><code>case class Config(multiplier: Int)\n\nval transform = Transform[Int, Int]\n  .requires[Config] { config =&gt; x =&gt;\n    x * config.multiplier\n  }\n\nval load = Load[Int, Unit]\n  .requires[Config] { config =&gt; x =&gt;\n    println(s\"Result with multiplier ${config.multiplier}: $x\")\n  }\n\nval pipeline = Extract(5) ~&gt; transform ~&gt; load\n\nval config = Config(multiplier = 3)\npipeline.provide(config).unsafeRun(())  // prints \"Result with multiplier 3: 15\"\n</code></pre> <p><code>.requires[Config]</code> declares a dependency. <code>.provide(config)</code> supplies it. No globals, no parameter drilling.</p>"},{"location":"first-pipeline/#thats-it","title":"That's it","text":"<p>You now know:</p> <ul> <li><code>Node</code> wraps functions</li> <li><code>~&gt;</code> chains nodes</li> <li><code>&amp;</code> runs nodes in parallel</li> <li><code>.requires</code> / <code>.provide</code> handles config</li> </ul> <p>Next: Core Concepts for more details, or Examples to see real usage with Spark/Flink.</p>"},{"location":"installation/","title":"Installation","text":"<p>etl4s is on MavenCentral and cross-built for Scala, 2.12, 2.13, 3.x: <pre><code>\"xyz.matthieucourt\" %% \"etl4s\" % \"1.9.0\"\n</code></pre></p> <p>Try it in your REPL (with scala-cli): <pre><code>scala-cli repl --scala 3 --dep xyz.matthieucourt:etl4s_3:1.9.0\n</code></pre> You can drop Etl4s.scala into any Scala project and use it like a header file</p>"},{"location":"lineage/","title":"Lineage","text":"<p>Attach lineage metadata with <code>.lineage</code> then use <code>.toDot</code>, <code>.toMermaid</code> or <code>.toJson</code> to get the string representation of your lineage diagrams.</p>"},{"location":"lineage/#quick-start","title":"Quick Start","text":"<pre><code>import etl4s._\n\nval A = Node[String, String](identity)\n  .lineage(\n    name = \"A\",\n    inputs = List(\"s1\", \"s2\"),\n    outputs = List(\"s3\"), \n    schedule = \"0 */2 * * *\"\n  )\n\nval B = Node[String, String](identity)\n  .lineage(\n    name = \"B\",\n    inputs = List(\"s3\"),\n    outputs = List(\"s4\", \"s5\")\n  )\n</code></pre> <p>Export to JSON, DOT (Graphviz), or Mermaid:</p> <pre><code>Seq(A, B).toJson\nSeq(A, B).toDot\nSeq(A, B).toMermaid\n</code></pre>"},{"location":"lineage/#visualization","title":"Visualization","text":""},{"location":"lineage/#dot","title":"DOT","text":"<p>Generate DOT graphs for Graphviz:</p> <pre><code>Seq(A, B).toDot\n</code></pre> <p> </p>"},{"location":"lineage/#mermaid","title":"Mermaid","text":"<pre><code>Seq(A, B).toMermaid\n</code></pre> <pre><code>graph LR\n    classDef pipeline fill:#e1f5fe,stroke:#01579b,stroke-width:2px,color:#000\n    classDef dataSource fill:#f3e5f5,stroke:#4a148c,stroke-width:2px,color:#000\n\n    A[\"A&lt;br/&gt;(0 */2 * * *)\"]\n    B[\"B\"]\n    s1([\"s1\"])\n    s2([\"s2\"])\n    s3([\"s3\"])\n    s4([\"s4\"])\n    s5([\"s5\"])\n\n    s1 --&gt; A\n    s2 --&gt; A\n    A --&gt; s3\n    s3 --&gt; B\n    B --&gt; s4\n    B --&gt; s5\n    A -.-&gt; B\n    linkStyle 6 stroke:#ff6b35,stroke-width:2px\n\n    class A pipeline\n    class B pipeline\n    class s1 dataSource\n    class s2 dataSource\n    class s3 dataSource\n    class s4 dataSource\n    class s5 dataSource</code></pre> <p>Orange dotted arrows show inferred dependencies.</p>"},{"location":"lineage/#json","title":"JSON","text":"<pre><code>Seq(A, B).toJson\n</code></pre> <p>JSON structure includes: - <code>pipelines</code>: Array of pipeline objects - <code>dataSources</code>: Array of data source names - <code>edges</code>: Connections with <code>isDependency</code> flag</p>"},{"location":"lineage/#lineage-parameters","title":"Lineage Parameters","text":"<ul> <li><code>name</code> (required): Unique identifier</li> <li><code>inputs</code>: Input data sources (default: empty)</li> <li><code>outputs</code>: Output data sources (default: empty)</li> <li><code>upstreams</code>: Explicit dependencies (Nodes, Readers, or Strings)</li> <li><code>schedule</code>: Human-readable schedule (e.g., \"0 /2 * * \")</li> <li><code>cluster</code>: Group name for organizing related pipelines</li> </ul>"},{"location":"lineage/#explicit-upstreams","title":"Explicit Upstreams","text":"<p>Use <code>upstreams</code> for non-data dependencies:</p> <p>If you add a node <code>C</code> <pre><code>val C = Node[String, String](identity)\n  .lineage(\"C\", upstreams = List(A, B))\n</code></pre></p> <p>Then do: <pre><code>Seq(A, B, C).toDot\n</code></pre></p> <p> </p> <p>Note how <code>C</code> has an orange upstream dependency to <code>A</code> and <code>B</code> despite not having as inputs their outputs.</p>"},{"location":"lineage/#clusters","title":"Clusters","text":"<p>Group related pipelines:</p> <pre><code>val B = Node[String, String](identity)\n  .lineage(\n    name = \"B\",\n    inputs = List(\"s3\"),\n    outputs = List(\"s4\", \"s5\"),\n    cluster = \"Y\"\n  )\n\nval C = Node[String, String](identity)\n  .lineage(\n    name = \"C\",\n    upstreams = List(A, B),\n    cluster = \"Y\"\n  )\n\nSeq(A, B, C).toDot\n</code></pre> <p> </p>"},{"location":"opentelemetry/","title":"Telemetry","text":"<p>When writing ETL jobs, you often need to:</p> <ul> <li>Count records processed, track durations, measure data quality</li> <li>Ship metrics to Prometheus, DataDog, or whatever your infra uses</li> <li>Have zero overhead in dev, real metrics in prod</li> </ul> <p><code>Tel</code> gives you this. It's a thin interface over your metrics backend. No-ops by default, wired up when you provide an implementation.</p> <pre><code>val process = Transform[List[String], Int] { data =&gt;\n  Tel.withSpan(\"processing\") {\n    Tel.addCounter(\"items\", data.size)\n    data.map(_.length).sum\n  }\n}\n\n/* Development: no-ops (zero cost) */\nprocess.unsafeRun(data)\n\n/* Production: your backend */\nimplicit val telemetry: Etl4sTelemetry = MyPrometheusProvider()\nprocess.unsafeRun(data)\n</code></pre>"},{"location":"opentelemetry/#quick-setup-env-based-telemetry","title":"Quick Setup: Env-Based Telemetry","text":"<p>A common pattern is to wire telemetry based on environment:</p> <pre><code>object TelemetryConfig {\n  implicit val telemetry: Etl4sTelemetry =\n    if (sys.env.getOrElse(\"ENV\", \"dev\") == \"prod\")\n      OpenTelemetryProvider()    /* Real metrics in prod */\n    else\n      Etl4sConsoleTelemetry()    /* Print to stdout in dev */\n}\n\n/* In your pipeline code */\nimport TelemetryConfig._\n\nval pipeline = extract ~&gt; process ~&gt; load\npipeline.unsafeRun()  /* Automatically uses the right backend */\n</code></pre>"},{"location":"opentelemetry/#the-interface","title":"The Interface","text":"<pre><code>trait Etl4sTelemetry {\n  def withSpan[T](name: String, attributes: (String, Any)*)(block: =&gt; T): T\n  def addCounter(name: String, value: Long): Unit\n  def setGauge(name: String, value: Double): Unit\n  def recordHistogram(name: String, value: Double): Unit\n}\n</code></pre> <p>Your implementation connects to OpenTelemetry SDK, Prometheus, DataDog, New Relic, CloudWatch, or whatever you use.</p>"},{"location":"opentelemetry/#why-telemetry-in-etl-business-logic","title":"Why Telemetry in ETL Business Logic","text":"<p>In web apps, telemetry is often a cross-cutting concern. ETL is different. In batch/streaming jobs, metrics are frequently business-critical:</p> <pre><code>val processUsers = Transform[List[RawUser], List[ValidUser]] { rawUsers =&gt;\n  val validated = rawUsers.filter(isValid)\n  val invalidCount = rawUsers.size - validated.size\n\n  /* These ARE business metrics */\n  Tel.addCounter(\"users.processed\", rawUsers.size)\n  Tel.addCounter(\"users.invalid\", invalidCount)\n  Tel.setGauge(\"data.quality.ratio\", validated.size.toDouble / rawUsers.size)\n\n  if (invalidCount &gt; threshold) {\n    Tel.addCounter(\"pipeline.quality.failures\", 1)\n    throw new DataQualityException(\"Too many invalid records\")\n  }\n\n  validated\n}\n</code></pre> <p>These aren't just \"monitoring metrics\" - they're business KPIs:</p> <ul> <li>Record counts determine billing and SLAs</li> <li>Data quality ratios trigger business alerts</li> <li>Throughput metrics inform capacity planning</li> </ul>"},{"location":"opentelemetry/#implementation-examples","title":"Implementation Examples","text":""},{"location":"opentelemetry/#opentelemetry-sdk","title":"OpenTelemetry SDK","text":"<pre><code>class OpenTelemetryProvider extends Etl4sTelemetry {\n  private val tracer = GlobalOpenTelemetry.getTracer(\"my-app\")\n  private val meter = GlobalOpenTelemetry.getMeter(\"my-app\")\n\n  def withSpan[T](name: String, attributes: (String, Any)*)(block: =&gt; T): T = {\n    val span = tracer.spanBuilder(name).startSpan()\n    try block finally span.end()\n  }\n\n  def addCounter(name: String, value: Long): Unit = {\n    meter.counterBuilder(name).build().add(value)\n  }\n  /* ... implement setGauge, recordHistogram */\n}\n</code></pre>"},{"location":"opentelemetry/#prometheus","title":"Prometheus","text":"<pre><code>class PrometheusProvider extends Etl4sTelemetry {\n  def withSpan[T](name: String, attributes: (String, Any)*)(block: =&gt; T): T = {\n    val timer = Timer.start()\n    try block finally histogram.labels(name).observe(timer.observeDuration())\n  }\n\n  def addCounter(name: String, value: Long): Unit = {\n    Counter.build().name(name).register().inc(value)\n  }\n  /* ... implement setGauge, recordHistogram */\n}\n</code></pre>"},{"location":"opentelemetry/#console-built-in","title":"Console (Built-in)","text":"<pre><code>/* Development telemetry - prints to stdout */\nimplicit val telemetry: Etl4sTelemetry = Etl4sConsoleTelemetry()\n</code></pre>"},{"location":"opentelemetry/#nested-spans","title":"Nested Spans","text":"<p>Spans automatically nest: <pre><code>Tel.withSpan(\"outer\") {\n  Tel.withSpan(\"inner\") {\n    computeResult()\n  }\n}\n</code></pre></p>"},{"location":"opentelemetry/#span-attributes","title":"Span Attributes","text":"<pre><code>Tel.withSpan(\"processing\",\n  \"input.size\" -&gt; data.size,\n  \"batch.id\" -&gt; batchId\n) {\n  /* processing logic */\n}\n</code></pre>"},{"location":"opentelemetry/#api-reference","title":"API Reference","text":""},{"location":"opentelemetry/#tel-object","title":"Tel Object","text":"Method Description <code>Tel.withSpan(name)(block)</code> Execute block in named span <code>Tel.addCounter(name, value)</code> Increment counter <code>Tel.setGauge(name, value)</code> Set gauge value <code>Tel.recordHistogram(name, value)</code> Record histogram value"},{"location":"opentelemetry/#built-in-implementations","title":"Built-in Implementations","text":"Implementation Description <code>Etl4sConsoleTelemetry()</code> Prints to stdout <code>Etl4sNoOpTelemetry</code> Silent no-op (default)"},{"location":"operators/","title":"Operators","text":"Operator Name What it does Result type <code>~&gt;</code> Chain <code>a ~&gt; b</code> - output of <code>a</code> feeds into <code>b</code> <code>B</code> <code>&amp;</code> Fan-out <code>a &amp; b</code> - run both sequentially, same input <code>(A, B)</code> <code>&amp;&gt;</code> Parallel <code>a &amp;&gt; b</code> - run both concurrently, same input <code>(A, B)</code> <code>&gt;&gt;</code> Sequence <code>a &gt;&gt; b</code> - run both in order, return <code>b</code>'s result <code>B</code> <code>.If</code> / <code>.ElseIf</code> / <code>.Else</code> Branch conditional routing varies"},{"location":"philosophy/","title":"Philosophy","text":""},{"location":"philosophy/#discipline-upon-assignment","title":"Discipline Upon Assignment","text":"<p>etl4s espouses the idea that is is beneficial to banish the assignment (<code>=</code>) operator at the key \"wiring\" stage of dataflow programs.</p> <p>Wiring via raw composition (<code>g andThen f</code> style) has limitations: chiefly, config and DI type-slots clutter call-sites and run orthogonal to dataflow) and wiring via monadic stacks don't impose a total discipline over the assingment operator and creating new bindings.</p> <p>Say we have <pre><code>for {\n  e1 &lt;- extract1\n  t1 &lt;- transform(e1)\n  l &lt;- load(t1)\n} yield ()\n</code></pre></p> <p>this is good, our eyes can in one top-down motion read: <code>e &gt; t &gt; l</code></p> <p>but begins the discipline over assignment and vertical dataflow is broken if we introduce bindings: <pre><code>for {\n  filterDate = ???\n  endDate = ???\n  e1 &lt;- extract1\n  startDate = ???\n  t1 &lt;- transform(e1, filterDate)\n  l &lt;- load(t1, startDate, endDate)\n} yield()\n</code></pre></p> <p>Our eyes now need to do: <code>filterDate &gt; endDate &gt; e &gt; startDate &gt; t &gt; (Back to filterDate) &gt; l &gt; (Back to startDate) &gt; (Back to endDate)</code></p>"},{"location":"philosophy/#linearizability-controlled-fan-out-with-reconvergence","title":"Linearizability (Controlled fan-out with reconvergence)","text":"<p>etl4s deliberately channels you into a linearized \"Function1 model\" of \"give me ONE input, I'll give you ONE output\".</p> <p>That said, it also lets you snap together pipelines with multiple input sources tupled together, easily fork-off conditional branches with heterogeneous types, and chain together side-outputs.</p> <p>The idea is to give the programmer clear little two ended pipes basically, not multi-sided puzzle pieces.</p> <p>Imagine we have: <pre><code>import et4ls._\n\nval p = (e1 &amp; e2) ~&gt; t ~&gt; log &gt;&gt; saveS3 &gt;&gt; .If(_ &gt; 0)(enrich ~&gt; dbLoad)\n                                           .Else(process ~&gt; purgatoryLoad)\n</code></pre></p> <p>Different branches can have different types and requirements, but once stitched together you have a single node that has intersected upstream, and unioned downstream the branch types.</p>"},{"location":"philosophy/#metrics-as-business-logic","title":"Metrics as business logic","text":"<p>In OLAP, observability metrics are not mere infrastructure concerns. Things like \"Records processed\", \"validation failures\", \"data quality scores\" tend to be part of your logic-proper.</p> <p>etl4s lets you write metrics inline with <code>Tel</code> calls. They are all zero-cost no-ops until you provide an implementation.</p>"},{"location":"philosophy/#what-etl4s-is-not","title":"What etl4s is NOT","text":"<p>Not a workflow orchestrator etl4s doesn't schedule jobs, or handle distributed coordination. Use Airflow or whatever flavour of scheduler for this. However, you will find that when your data </p> <p>Not a data processing engine etl4s doesn't move data or execute transformations. Use Spark, Flink, Pandas for that. etl4s makes your Spark/Flink job logic composable and type-safe.</p> <p>Not a replacement for monadic IO with fiber runtimes If you're already using Cats Effect or ZIO, you probably do not need etl4s (although in the near the concurrency subsystem will not be tied to <code>Future</code> so you will be able to run etl4s on top of CE, ZIO, Kyo). It's for teams that want structure without committing to an effect system and learning its abstractions.</p>"},{"location":"tasks/","title":"Parallel Execution","text":"<p>etl4s has an elegant shorthand for grouping and parallelizing operations that share the same input type: <pre><code>import etl4s._\n\n/* Simulate slow IO operations (e.g: DB calls, API requests) */\n\nval e1 = Extract { Thread.sleep(100); 42 }\nval e2 = Extract { Thread.sleep(100); \"hello\" }\nval e3 = Extract { Thread.sleep(100); true }\n</code></pre></p> <p>Sequential run of e1, e2, and e3 (~300ms total) <pre><code>val sequential: Extract[Unit, (Int, String, Boolean)] =\n     e1 &amp; e2 &amp; e3\n</code></pre></p> <p>Parallel run of e1, e2, e3 on their own JVM threads with Scala Futures (~100ms total, same result, 3X faster) <pre><code>import scala.concurrent.ExecutionContext.Implicits.global\n\nval parallel: Extract[Unit, (Int, String, Boolean)] =\n     e1 &amp;&gt; e2 &amp;&gt; e3\n</code></pre></p> <p>Mix sequential and parallel execution (first two parallel (~100ms), then third (~100ms)): <pre><code>val mixed = (e1 &amp;&gt; e2) &amp; e3\n</code></pre></p> <p>Full example of a parallel pipeline: <pre><code>val consoleLoad: Load[String, Unit] = Load(println(_))\nval dbLoad:      Load[String, Unit] = Load(x =&gt; println(s\"DB Load: ${x}\"))\n\nval merge = Transform[(Int, String, Boolean), String] { case (i, s, b) =&gt;\n    s\"$i-$s-$b\"\n  }\n\nval pipeline =\n  (e1 &amp;&gt; e2 &amp;&gt; e3) ~&gt; merge ~&gt; (consoleLoad &amp;&gt; dbLoad)\n</code></pre></p>"},{"location":"testing/","title":"Testing","text":"<p>Use etl4s with the testing framework of your choice</p> <p>Run nodes like normal functions <pre><code>import etl4s._\n\nval times5: Transform[Int, Int] = Transform(_ * 5)\n\ntimes5(5)\n</code></pre></p> <p>You will get: <pre><code>25\n</code></pre></p> <p>Run pipelines with <code>unsafeRun</code> or <code>safeRun</code>:</p> <p><pre><code>import etl4s._\n\nval plus2:  Transform[Int, Int] = Transform(_ + 2)\nval times5: Transform[Int, Int] = Transform(_ * 5)\n\nval p: Pipeline[Int, Int] = plus2 ~&gt; times5\n\np.unsafeRun(2)\n</code></pre> Gives <pre><code>20\n</code></pre> However, if you use <code>safeRun</code> as below <pre><code>p.safeRun(2)\n</code></pre> You will get a response wrapped in a <code>scala.util.Try</code> <pre><code>Success(20)\n</code></pre></p>"},{"location":"testing/#testing-with-traces","title":"Testing with Traces","text":"<p>For testing with execution insights, see the Pipeline Tracing section. You can test traced execution and cross-node communication:</p> <pre><code>import etl4s._\n\nval pipeline = Transform[String, Int](_.length)\nval trace = pipeline.unsafeRunTrace(\"test\")\n\nassert(trace.result == 4)\nassert(trace.timeElapsedMillis &gt;= 0)\nassert(!trace.hasErrors)\n</code></pre>"},{"location":"trace/","title":"Tracing","text":"<p>When writing dataflows, you often want to:</p> <ul> <li>Log what's happening at each step</li> <li>Have downstream nodes react to upstream failures</li> <li>Get timing and debug info after execution</li> </ul> <p><code>Trace</code> is a shared, append-only log that flows through your pipeline. Nodes can write to it, read from it, and react to what happened upstream.</p> <pre><code>val A = Transform[String, Int] { s =&gt;\n  Trace.log(\"Processing\")\n  s.length\n}\n\nval res: Int = A.unsafeRun(\"hello\")  // 5\nval trace: Trace[Int] = A.unsafeRunTrace(\"hello\")\n</code></pre> <pre><code>Trace(\n  result = 5,\n  logs = List(\"Processing\"),\n  errors = List(),\n  timeElapsedMillis = 2L\n)\n</code></pre> <p>Each pipeline run initializes fresh ThreadLocal state, shared by all nodes in that execution, cleared on completion.</p>"},{"location":"trace/#nodes-that-react-to-each-other","title":"Nodes That React to Each Other","text":"<p>Downstream nodes can see what happened upstream:</p> <pre><code>val A = Transform[String, Int] { s =&gt;\n  if (s.isEmpty) Trace.error(\"empty\")\n  s.length\n}\n\nval B = Transform[Int, String] { n =&gt;\n  if (Trace.hasErrors) \"FALLBACK\" else s\"len: $n\"\n}\n\nval pipeline = A ~&gt; B\n\npipeline.unsafeRun(\"hello\")  // \"len: 5\"\npipeline.unsafeRun(\"\")       // \"FALLBACK\"\n</code></pre> <p>No wiring required. <code>B</code> checks <code>Trace.hasErrors</code> and switches to fallback mode.</p>"},{"location":"trace/#live-pipeline-state","title":"Live Pipeline State","text":"<p>Check elapsed time, error counts, etc. mid-execution:</p> <pre><code>val p = Transform[String, String] { input =&gt;\n  if (Trace.getElapsedTimeMillis &gt; 1000) {\n    \"TIMEOUT\"\n  } else {\n    input.toUpperCase\n  }\n}\n</code></pre>"},{"location":"trace/#quick-reference","title":"Quick Reference","text":""},{"location":"trace/#write","title":"Write","text":"Method Description <code>Trace.log(message)</code> Log any value <code>Trace.error(err)</code> Log an error"},{"location":"trace/#check","title":"Check","text":"Method Description <code>Trace.hasErrors</code> Any errors so far? <code>Trace.hasLogs</code> Any logs so far?"},{"location":"trace/#read","title":"Read","text":"Method Description <code>Trace.getCurrent</code> Full current state <code>Trace.getLogs</code> All logs <code>Trace.getErrors</code> All errors <code>Trace.getElapsedTimeMillis</code> Time since start <code>Trace.getLogCount</code> Number of logs <code>Trace.getErrorCount</code> Number of errors <code>Trace.getLastLog</code> Most recent log <code>Trace.getLastError</code> Most recent error"},{"location":"trace/#trace-result","title":"Trace Result","text":"<p>After calling <code>.unsafeRunTrace()</code> or <code>.safeRunTrace()</code>:</p> Property Type Description <code>result</code> <code>A</code> or <code>Try[A]</code> Execution result <code>logs</code> <code>List[Any]</code> All logged values <code>errors</code> <code>List[Any]</code> All errors <code>timeElapsedMillis</code> <code>Long</code> Total execution time <code>hasErrors</code> <code>Boolean</code> Quick error check"},{"location":"tradeoffs/","title":"Tradeoffs","text":""},{"location":"tradeoffs/#tracing","title":"Tracing","text":"<p><code>Trace</code> uses two <code>ThreadLocal</code> lists - one for logs, one for errors. Appending is O(1) but not zero cost. If you're doing <code>Trace.log()</code> in a tight loop over millions of records, you're allocating. For normal ETL granularity (log per stage, per batch, per failure), you won't notice.</p>"},{"location":"tradeoffs/#parallelism","title":"Parallelism","text":"<p><code>&amp;&gt;</code> uses <code>Future</code> under the hood (for now). You bring the <code>ExecutionContext</code>:</p> <pre><code>import scala.concurrent.ExecutionContext.Implicits.global\n\nval parallel = e1 &amp;&gt; e2 &amp;&gt; e3\n</code></pre> <p>Keep in mind</p> <ul> <li>Each <code>&amp;&gt;</code> branch submits a <code>Future</code></li> <li>So avoid folding over some interrable of size <code>n</code> with <code>&amp;&gt;</code> since it would fire off a syscall for an OS thread <code>n</code> number of times</li> </ul> <p>The plan is to make an effect polymorphic etl4s concurrency subsystem (soon) ...so you could plug in ZIO, CE, Kyo or keep <code>Future</code>.</p>"},{"location":"tradeoffs/#telemetry","title":"Telemetry","text":"<p><code>Tel</code> compiles to no-ops when there's no <code>Etl4sTelemetry</code> in implicit scope. Zero allocation, zero overhead.</p>"},{"location":"validation/","title":"Ensurers","text":"<p>When writing dataflows, you often want to validate inputs and outputs at runtime - and reuse those validations across nodes, collecting all errors instead of failing on the first.</p> <p><code>.ensure()</code> lets you attach validators to any Node:</p> <pre><code>val process = Node[Int, String](n =&gt; s\"Value: $n\")\n  .ensure(\n    input  = Seq(isPositive, lessThan1k),\n    output = Seq(notEmpty)\n  )\n\nprocess.unsafeRun(42)   // \"Value: 42\"\nprocess.unsafeRun(-5)   // throws ValidationException: \"Must be positive\"\n</code></pre> <p>Validators are just functions <code>A =&gt; Option[String]</code>. Return <code>None</code> if valid, <code>Some(\"error message\")</code> if not:</p> <pre><code>val isPositive = (x: Int) =&gt; if (x &gt; 0) None else Some(\"Must be positive\")\nval lessThan1k = (x: Int) =&gt; if (x &lt; 1000) None else Some(\"Must be &lt; 1000\")\nval notEmpty   = (s: String) =&gt; if (s.nonEmpty) None else Some(\"Cannot be empty\")\n</code></pre>"},{"location":"validation/#change-validation","title":"Change Validation","text":"<p>Validate by examining both input and output together. The <code>change</code> validator receives a tuple <code>(input, output)</code>:</p> <pre><code>/* Ensure deduplication never grows the list */\nval noGrowth: ((List[Int], List[Int])) =&gt; Option[String] = {\n  case (in, out) =&gt;\n    if (out.size &lt;= in.size) None\n    else Some(s\"Output grew: ${in.size} -&gt; ${out.size}\")\n}\n\nval dedupe = Node[List[Int], List[Int]](_.distinct)\n  .ensure(change = Seq(noGrowth))\n\ndedupe.unsafeRun(List(1, 2, 2, 3))  // List(1, 2, 3) - valid, shrunk\n</code></pre>"},{"location":"validation/#error-accumulation","title":"Error Accumulation","text":"<p>Multiple failures are collected:</p> <pre><code>val validate = Node[Int, Int](identity)\n  .ensure(input = Seq(isPositive, lessThan100, isEven))\n\nvalidate.unsafeRun(-5)\n// ValidationException: \"Input validation failed:\n//   - Must be positive\n//   - Must be even\"\n</code></pre>"},{"location":"validation/#parallel-validation","title":"Parallel Validation","text":"<p>Use <code>.ensurePar()</code> to run expensive checks concurrently.</p>"},{"location":"validation/#trace-integration","title":"Trace Integration","text":"<p>Validation failures are logged to Trace:</p> <pre><code>val node = Node[Int, String](_.toString)\n  .ensure(input = Seq(isPositive))\n\nval trace = node.safeRunTrace(-5)\ntrace.errors.head  // \"Input validation failed: Must be positive\"\n</code></pre>"},{"location":"validation/#config-aware-validation","title":"Config-Aware Validation","text":"<p>Ensurers work on config nodes too. Validators are curried <code>Config =&gt; A =&gt; Option[String]</code> so they can access config:</p> <pre><code>case class Config(minValue: Int, maxValue: Int)\n\nval inRange: Config =&gt; Int =&gt; Option[String] = cfg =&gt; n =&gt;\n  if (n &gt;= cfg.minValue &amp;&amp; n &lt;= cfg.maxValue) None\n  else Some(s\"Must be between ${cfg.minValue} and ${cfg.maxValue}\")\n\nval process = Transform[Int, Int].requires[Config] { cfg =&gt; n =&gt; n * 2 }\n  .ensure(input = Seq(inRange))\n\nprocess.provide(Config(0, 100)).unsafeRun(50)   // 100\nprocess.provide(Config(0, 100)).unsafeRun(150)  // throws ValidationException\n</code></pre>"}]}