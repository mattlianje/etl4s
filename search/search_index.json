{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"etl4s <p>Powerful, whiteboard-style ETL</p> TRY IT! GET STARTED    GITHUB  <pre><code>import etl4s._\n\n/* Define your building blocks */\nval fiveExtract = Extract(5)\nval timesTwo    = Transform[Int, Int](_ * 2)\nval exclaim     = Transform[Int, String](n =&gt; s\"$n!\")\nval consoleLoad = Load[String, Unit](println(_))\n\n/* Add config with .requires */\nval dbLoad      = Load[String, Unit].requires[String] { dbType =&gt; s =&gt;\n  println(s\"Saving to $dbType DB: $s\")\n}\n\n/* Stitch your pipeline */\nval pipeline =\n  fiveExtract ~&gt; timesTwo ~&gt; exclaim ~&gt; (consoleLoad &amp; dbLoad)\n\n/* Provide config, then run*/\npipeline.provide(\"sqlite\").unsafeRun(())\n</code></pre> Whiteboard-style <p>Model pipelines like you think - with visual, readable chaining</p> Config-driven <p>Your pipelines are typed, declarative endpoints - easy to compose and trigger</p> Type-safe <p>Prevent bugs by catching type mismatches at compile time</p>"},{"location":"config/","title":"Configuration","text":"<p>Make your pipelines configurable and reusable. Declare what each step needs with <code>.requires</code>, then provide config once. The required context is automatically inferred.</p> <pre><code>import etl4s._\n\ncase class Cfg(key: String)\n\nval A = Extract(\"data\")\nval B = Transform[String, String].requires[Cfg] { cfg =&gt; data =&gt;\n  s\"${cfg.key}: $data\"\n}\n\nval pipeline = A ~&gt; B\n\npipeline.provide(Cfg(\"secret\")).unsafeRun(())  // \"secret: data\"\n</code></pre>"},{"location":"config/#config-propagation","title":"Config Propagation","text":"<p>Build modular configs with traits. etl4s automatically infers what your pipeline needs:</p> <pre><code>trait HasDb { def dbUrl: String }\ntrait HasAuth { def apiKey: String }\n\nval A = Load[String, Unit].requires[HasDb] { cfg =&gt; data =&gt;\n  println(s\"Saving to ${cfg.dbUrl}: $data\")\n}\n\nval B = Extract[Unit, String].requires[HasAuth] { cfg =&gt; _ =&gt;\n  s\"Fetched with ${cfg.apiKey}\"\n}\n\nval C = Transform[String, String](_.toUpperCase)\n\n// Combined config\ncase class AppConfig(dbUrl: String, apiKey: String) extends HasDb with HasAuth\n\nval pipeline = B ~&gt; C ~&gt; A\n\n// Config flows to all steps automatically\npipeline.provide(AppConfig(\"jdbc:pg\", \"secret-key\")).unsafeRun(())\n</code></pre>"},{"location":"config/#context","title":"Context","text":"<p><code>Context[T]</code> is a trait that provides organized factory methods for config-driven operations. For larger applications, extend it to keep your context-aware pipelines organized:</p> <pre><code>case class DbConfig(url: String, timeout: Int)\n\nobject DataPipeline extends Context[DbConfig] {\n\n  val fetch = Context.Extract[Unit, String] { cfg =&gt; _ =&gt;\n    s\"Connected to ${cfg.url} with timeout ${cfg.timeout}s\"\n  }\n\n  val save = Context.Load[String, Unit] { cfg =&gt; data =&gt;\n    println(s\"Saving to ${cfg.url}: $data\")\n  }\n\n  val pipeline = fetch ~&gt; save\n}\n\n// Provide config once\nDataPipeline.pipeline.provide(DbConfig(\"jdbc:pg\", 5000)).unsafeRun(())\n</code></pre>"},{"location":"config/#scala-2x-note","title":"Scala 2.x Note","text":"<p>Use explicit types for better inference:</p> <pre><code>// Scala 2.x\nTransform.requires[Config, String, String] { cfg =&gt; input =&gt; \n  cfg.key + input\n}\n\n// Scala 3 (preferred)\nTransform[String, String].requires[Config] { cfg =&gt; input =&gt; \n  cfg.key + input\n}\n</code></pre>"},{"location":"core-concepts/","title":"Core Concepts","text":"<p>etl4s has one core building block: <pre><code>Node[-In, +Out]\n</code></pre> A Node wraps a lazily-evaluated function <code>In =&gt; Out</code>. Chain them with <code>~&gt;</code> to build pipelines.</p>"},{"location":"core-concepts/#node-types","title":"Node types","text":"<p>To improve readability and express intent, etl4s defines four aliases: <code>Extract</code>, <code>Transform</code>, <code>Load</code> and <code>Pipeline</code>. All behave the same under the hood.</p> <pre><code>type Extract[-In, +Out]   = Node[In, Out]\ntype Transform[-In, +Out] = Node[In, Out]\ntype Load[-In, +Out]      = Node[In, Out]\ntype Pipeline[-In, +Out]  = Node[In, Out]\n</code></pre>"},{"location":"core-concepts/#building-pipelines","title":"Building pipelines","text":"<pre><code>import etl4s._\n\nval A = Extract(\"users.csv\")\nval B = Transform[String, Int](csv =&gt; csv.split(\"\\n\").length)\nval C = Load[Int, Unit](count =&gt; println(s\"Processed $count users\"))\n\nval pipeline = A ~&gt; B ~&gt; C\n\npipeline(())  // Processed 3 users\n</code></pre> <p>Create standalone nodes: <pre><code>val toUpper = Transform[String, String](_.toUpperCase)\ntoUpper(\"hello\")  // HELLO\n</code></pre></p>"},{"location":"core-concepts/#running-pipelines","title":"Running pipelines","text":"<p>Call like a function: <pre><code>pipeline(())\n</code></pre></p> <p>Or be explicit: <pre><code>pipeline.unsafeRun(())\n</code></pre></p> <p>Error handling: <pre><code>val risky = Pipeline[String, Int](_.toInt)\n\nrisky.safeRun(\"42\")    // Success(42)\nrisky.safeRun(\"oops\")  // Failure(...)\n</code></pre></p> <p>Execution details: <pre><code>val trace = pipeline.unsafeRunTrace(())\n// trace.result, trace.logs, trace.timeElapsedMillis, trace.errors\n\nval safeTrace = pipeline.safeRunTrace(())\n// safeTrace.result is a Try[Out]\n</code></pre></p> <p>\u26a0\ufe0f etl4s also has a <code>Reader</code> type for dependency injection. Use <code>.requires</code> to turn any Node into a <code>Reader[Config, Node]</code>. The <code>~&gt;</code> operator works between Nodes and Readers. See Configuration for details.</p>"},{"location":"effect/","title":"Side Effects with <code>tap</code>","text":"<p>The <code>tap</code> method lets you perform side effects without disrupting the data flow through your pipeline. Perfect for logging, cleanup, debugging, or any operation that shouldn't affect your data.</p> <pre><code>import etl4s._\n\nval fetchData    = Extract((_: Unit) =&gt; List(\"file1.txt\", \"file2.txt\"))\nval cleanup      = tap[List[String]] { files =&gt; \n  println(s\"Processing ${files.size} files...\")\n  cleanupTempFiles(files)\n}\nval processFiles = Transform[List[String], Int](_.size)\n\nval p = fetchData ~&gt; cleanup ~&gt; processFiles\n</code></pre>"},{"location":"effect/#using-nodeeffect-for-unit-operations","title":"Using <code>Node.effect</code> for Unit Operations","text":"<p>For pure side effects that don't need the pipeline data, use <code>Node.effect</code>:</p> <pre><code>val clearCache = Node.effect { println(\"Clearing cache...\") }\nval purgeTemp  = Node.effect { cleanupTempFiles() }\n\nval p = clearCache &gt;&gt; purgeTemp &gt;&gt; fetchData ~&gt; processFiles\n</code></pre>"},{"location":"examples/","title":"Examples","text":""},{"location":"examples/#chain-two-pipelines","title":"Chain two pipelines","text":"<pre><code>import etl4s._\n\nval A = Pipeline((i: Int) =&gt; i.toString)\nval B = Pipeline((s: String) =&gt; s + \"!\")\n\nval C = A ~&gt; B  // Int =&gt; String\n</code></pre>"},{"location":"examples/#complex-chaining","title":"Complex chaining","text":"<pre><code>import etl4s._\n\nval A = Pipeline(\"data\")\nval B = Pipeline(42)\nval C = Transform[String, String](_.toUpperCase)\nval D = Load[String, Unit](println)\n\nval pipeline =\n  for {\n    str &lt;- A\n    num &lt;- B\n    _ &lt;- Extract(s\"$str-$num\") ~&gt; C ~&gt; D\n  } yield ()\n</code></pre>"},{"location":"failures/","title":"Handling failures","text":"<p>etl4s provides built-in failure handling:</p>"},{"location":"failures/#withretry","title":".withRetry","text":"<p>Retry failed operations with exponential backoff using <code>.withRetry</code>: <pre><code>import etl4s._\n\nvar attempts = 0\n\nval riskyTransformWithRetry = Transform[Int, String] {\n    n =&gt;\n      attempts += 1\n      if (attempts &lt; 3) throw new RuntimeException(s\"Attempt $attempts failed\")\n      else s\"Success after $attempts attempts\"\n}.withRetry(maxAttempts = 3, initialDelayMs = 10)\n\nval pipeline = Extract(42) ~&gt; riskyTransformWithRetry\npipeline.unsafeRun(())\n</code></pre> Output: <pre><code>Success after 3 attempts\n</code></pre></p>"},{"location":"failures/#onfailure","title":".onFailure","text":"<p>Catch exceptions and provide fallback values using <code>.onFailure</code>: <pre><code>import etl4s._\n\nval riskyExtract =\n    Extract[Unit, String](_ =&gt; throw new RuntimeException(\"Boom!\"))\n\nval safeExtract = riskyExtract.onFailure(e =&gt; s\"Failed: ${e.getMessage}\")\nval consoleLoad = Load[String, Unit](println(_))\n\nval pipeline = safeExtract ~&gt; consoleLoad\npipeline.unsafeRun(())\n</code></pre> Output: <pre><code>Failed: Boom!\n</code></pre></p>"},{"location":"first-pipeline/","title":"Your First Pipeline","text":"<p>Build a complete ETL pipeline in minutes. We'll use Spark, but etl4s works with any data processing library.</p> <pre><code>scala-cli repl \\\n  --dep xyz.matthieucourt::etl4s:1.4.1 \\\n  --dep org.apache.spark:spark-sql_2.13:3.5.0\n</code></pre>"},{"location":"first-pipeline/#basic-pipeline","title":"Basic Pipeline","text":"<pre><code>import etl4s._\nimport org.apache.spark.sql.{SparkSession, DataFrame}\n\nval spark = SparkSession.builder().appName(\"etl4s\").master(\"local[*]\").getOrCreate()\nimport spark.implicits._\n\n/* Sample data */\nval usersDF = Seq(\n  (1, \"Alice\", \"alice@example.com\", 25, \"2023-01-15\", true),\n  (2, \"Bob\", \"bob@example.com\", 32, \"2023-03-22\", true),\n  (3, \"Charlie\", \"charlie@example.com\", 19, \"2022-11-08\", false)\n).toDF(\"id\", \"name\", \"email\", \"age\", \"register_date\", \"active\")\n\n/* Pipeline components */\nval extract = Extract[Unit, DataFrame](_ =&gt; usersDF)\nval filter = Transform[DataFrame, DataFrame](_.filter(\"active = true\"))\nval report = Load[DataFrame, Unit](_.show())\n\n/* Compose and run */\nval pipeline = extract ~&gt; filter ~&gt; report\npipeline.unsafeRun(())\n</code></pre>"},{"location":"first-pipeline/#config-driven-pipeline","title":"Config-Driven Pipeline","text":"<p>Make your pipelines configurable and reusable:</p> <pre><code>case class Config(minAge: Int, outputPath: String)\n\nval extract = Extract[Unit, DataFrame].requires[Config] { cfg =&gt; _ =&gt;\n  usersDF.filter(col(\"age\") &gt;= cfg.minAge)\n}\n\nval save = Load[DataFrame, Unit].requires[Config] { cfg =&gt; df =&gt;\n  println(s\"Saving to ${cfg.outputPath}\")\n  df.show()\n}\n\nval pipeline = extract ~&gt; save\n\n/* Provide config and run */\nval config = Config(minAge = 25, outputPath = \"data/users\")\npipeline.provide(config).unsafeRun(())\n</code></pre> <p>That's it! You've built a complete, configurable ETL pipeline with type safety and clean composition.</p>"},{"location":"installation/","title":"Installation","text":"<p>etl4s is on MavenCentral and cross-built for Scala, 2.12, 2.13, 3.x: <pre><code>\"xyz.matthieucourt\" %% \"etl4s\" % \"1.6.0\"\n</code></pre></p> <p>Try it in your REPL (with scala-cli): <pre><code>scala-cli repl --scala 3 --dep xyz.matthieucourt:etl4s_3:1.6.0\n</code></pre> You can drop Etl4s.scala into any Scala project and use it like a header file</p>"},{"location":"lineage/","title":"Lineage","text":"<p>Attach lineage metadata with <code>.lineage</code> then use <code>.toDot</code>, <code>.toMermaid</code> or <code>.toJson</code> to get the string representation of your lineage diagrams.</p>"},{"location":"lineage/#quick-start","title":"Quick Start","text":"<pre><code>import etl4s._\n\nval A = Node[String, String](identity)\n  .lineage(\n    name = \"A\",\n    inputs = List(\"s1\", \"s2\"),\n    outputs = List(\"s3\"), \n    schedule = Some(\"0 */2 * * *\")\n  )\n\nval B = Node[String, String](identity)\n  .lineage(\n    name = \"B\",\n    inputs = List(\"s3\"),\n    outputs = List(\"s4\", \"s5\")\n  )\n</code></pre> <p>Export to JSON, DOT (Graphviz), or Mermaid:</p> <pre><code>Seq(A, B).toJson\nSeq(A, B).toDot\nSeq(A, B).toMermaid\n</code></pre>"},{"location":"lineage/#visualization","title":"Visualization","text":""},{"location":"lineage/#dot","title":"DOT","text":"<p>Generate DOT graphs for Graphviz:</p> <pre><code>Seq(A, B).toDot\n</code></pre> <p> </p>"},{"location":"lineage/#mermaid","title":"Mermaid","text":"<pre><code>Seq(A, B).toMermaid\n</code></pre> <pre><code>graph LR\n    classDef pipeline fill:#e1f5fe,stroke:#01579b,stroke-width:2px,color:#000\n    classDef dataSource fill:#f3e5f5,stroke:#4a148c,stroke-width:2px,color:#000\n\n    A[\"A&lt;br/&gt;(0 */2 * * *)\"]\n    B[\"B\"]\n    s1([\"s1\"])\n    s2([\"s2\"])\n    s3([\"s3\"])\n    s4([\"s4\"])\n    s5([\"s5\"])\n\n    s1 --&gt; A\n    s2 --&gt; A\n    A --&gt; s3\n    s3 --&gt; B\n    B --&gt; s4\n    B --&gt; s5\n    A -.-&gt; B\n    linkStyle 6 stroke:#ff6b35,stroke-width:2px\n\n    class A pipeline\n    class B pipeline\n    class s1 dataSource\n    class s2 dataSource\n    class s3 dataSource\n    class s4 dataSource\n    class s5 dataSource</code></pre> <p>Orange dotted arrows show inferred dependencies.</p>"},{"location":"lineage/#json","title":"JSON","text":"<pre><code>Seq(A, B).toJson\n</code></pre> <p>JSON structure includes: - <code>pipelines</code>: Array of pipeline objects - <code>dataSources</code>: Array of data source names - <code>edges</code>: Connections with <code>isDependency</code> flag</p>"},{"location":"lineage/#lineage-parameters","title":"Lineage Parameters","text":"<ul> <li><code>name</code> (required): Unique identifier</li> <li><code>inputs</code>: Input data sources (default: empty)</li> <li><code>outputs</code>: Output data sources (default: empty)</li> <li><code>upstreams</code>: Explicit dependencies (Nodes, Readers, or Strings)</li> <li><code>schedule</code>: Human-readable schedule (e.g., \"0 /2 * * \")</li> <li><code>cluster</code>: Group name for organizing related pipelines</li> </ul>"},{"location":"lineage/#explicit-upstreams","title":"Explicit Upstreams","text":"<p>Use <code>upstreams</code> for non-data dependencies:</p> <p>If you add a node <code>C</code> <pre><code>val C = Node[String, String](identity)\n  .lineage(\"C\", upstreams = List(A, B))\n</code></pre></p> <p>Then do: <pre><code>Seq(A, B, C).toDot\n</code></pre></p> <p> </p> <p>Note how <code>C</code> has an orange upstream dependency to <code>A</code> and <code>B</code> despite not having as inputs their outputs.</p>"},{"location":"lineage/#clusters","title":"Clusters","text":"<p>Group related pipelines:</p> <pre><code>val B = Node[String, String](identity)\n  .lineage(\n    name = \"B\",\n    inputs = List(\"s3\"),\n    outputs = List(\"s4\", \"s5\"),\n    cluster = Some(\"Y\")\n  )\n\nval C = Node[String, String](identity)\n  .lineage(\n    name = \"C\",\n    upstreams = List(A, B),\n    cluster = Some(\"Y\")\n  )\n\nSeq(A, B, C).toDot\n</code></pre> <p> </p>"},{"location":"logs/","title":"Side Outputs","text":"<p>The <code>tap</code> method allows you to observe values flowing through your pipeline without modifying them.  This is useful for logging, debugging, or collecting metrics.</p> <p><pre><code>import etl4s._\n\nval sayHello   = Extract(\"hello world\")\nval splitWords = Transform[String, Array[String]](_.split(\" \"))\n\nval pipeline = sayHello ~&gt; \n               tap((x: String) =&gt; println(s\"Processing: $x\")) ~&gt;\n               splitWords\n\nval result = pipeline.unsafeRun(())\n</code></pre> This will return <code>Array(\"hello\", \"world\")</code> and also prints to stdout <code>Processing: hello world</code></p>"},{"location":"opentelemetry/","title":"Telemetry","text":"<p>etl4s provides a minimal telemetry interface. This interface exists to decouple telemetry from specific backends.</p>"},{"location":"opentelemetry/#how-it-works","title":"How It Works","text":"<p>The <code>Etl4sTelemetry</code> trait defines the core interface:</p> <pre><code>trait Etl4sTelemetry {\n  def withSpan[T](name: String, attributes: (String, Any)*)(block: =&gt; T): T\n  def addCounter(name: String, value: Long): Unit\n  def setGauge(name: String, value: Double): Unit  \n  def recordHistogram(name: String, value: Double): Unit\n}\n</code></pre> <p>All etl4s pipeline run methods automatically look for <code>Etl4sTelemetry</code> in implicit scope.</p> <p>The <code>Tel</code> object provides a convenient API with identical method names to the trait. By default, all <code>Tel</code> calls are no-ops with zero overhead until you provide an implementation.</p> <p>Your implementation connects to: OpenTelemetry SDK, Prometheus, DataDog, New Relic, CloudWatch, or whatever you want.</p>"},{"location":"opentelemetry/#usage","title":"Usage","text":"<pre><code>val process = Transform[List[String], Int] { data =&gt;\n  Tel.withSpan(\"processing\") {\n    Tel.addCounter(\"items\", data.size)\n    data.map(_.length).sum\n  }\n}\n\n/* Development: no-ops (zero cost) */\nprocess.unsafeRun(data)\n\n/* Production: your backend */\nimplicit val telemetry: Etl4sTelemetry = MyPrometheusProvider()\nprocess.unsafeRun(data)\n</code></pre> <p>Key benefits:</p> <ul> <li>Write business critical telemetry in business logic, not infrastructure code</li> <li>Zero performance cost until enabled  </li> <li>Works on any platform: local JVM, Spark, Kubernetes, Lambda</li> <li>No framework lock-in or context threading</li> </ul>"},{"location":"opentelemetry/#why-telemetry-sometimes-belongs-in-etl-business-logic","title":"Why Telemetry (sometimes) belongs in ETL Business Logic","text":"<p>In many web and OLTP programs, telemetry is often a cross-cutting concern separate from business logic. ETL is different. In OLAP processes, observability metrics are frequently business-critical (especially at the peripheries in Extractors and Loaders):</p> <pre><code>val processUsers = Transform[List[RawUser], List[ValidUser]] { rawUsers =&gt;\n  val validated = rawUsers.filter(isValid)\n  val invalidCount = rawUsers.size - validated.size\n\n  /* This IS business logic - the business needs these metrics */\n  Tel.addCounter(\"users.processed\", rawUsers.size) \n  Tel.addCounter(\"users.invalid\", invalidCount)\n  Tel.setGauge(\"data.quality.ratio\", validated.size.toDouble / rawUsers.size)\n\n  /* Business decision based on data quality */\n  if (invalidCount &gt; threshold) {\n    Tel.addCounter(\"pipeline.quality.failures\", 1)\n    throw new DataQualityException(\"Too many invalid records\")\n  }\n\n  validated\n}\n</code></pre> <p>In the above example - these aren't just \"monitoring metrics\" - they're business KPIs:</p> <ul> <li>Record counts determine billing and SLAs</li> <li>Processing times affect customer experience  </li> <li>Data quality ratios trigger business alerts</li> <li>Throughput metrics inform capacity planning</li> </ul> <p>etl4s makes it safe to instrument business logic directly because <code>Tel</code> calls are zero-cost no-ops by default. You get the observability where it matters most - in the business context - without infrastructure coupling.</p>"},{"location":"opentelemetry/#implementation-examples","title":"Implementation Examples","text":""},{"location":"opentelemetry/#opentelemetry-sdk","title":"OpenTelemetry SDK","text":"<pre><code>class OpenTelemetryProvider extends Etl4sTelemetry {\n  private val tracer = GlobalOpenTelemetry.getTracer(\"my-app\")\n  private val meter = GlobalOpenTelemetry.getMeter(\"my-app\")\n\n  def withSpan[T](name: String, attributes: (String, Any)*)(block: =&gt; T): T = {\n    val span = tracer.spanBuilder(name).startSpan()\n    try block finally span.end()\n  }\n\n  def addCounter(name: String, value: Long): Unit = {\n    meter.counterBuilder(name).build().add(value)\n  }\n  /* ... implement setGauge, recordHistogram */\n}\n</code></pre>"},{"location":"opentelemetry/#prometheus","title":"Prometheus","text":"<pre><code>class PrometheusProvider extends Etl4sTelemetry {\n  def withSpan[T](name: String, attributes: (String, Any)*)(block: =&gt; T): T = {\n    val timer = Timer.start()\n    try block finally histogram.labels(name).observe(timer.observeDuration())\n  }\n\n  def addCounter(name: String, value: Long): Unit = {\n    Counter.build().name(name).register().inc(value)\n  }\n  /* ... implement setGauge, recordHistogram */\n}\n</code></pre>"},{"location":"opentelemetry/#console-built-in","title":"Console (Built-in)","text":"<pre><code>/* Development telemetry - prints to stdout */\nimplicit val telemetry: Etl4sTelemetry = Etl4sConsoleTelemetry()\n</code></pre>"},{"location":"opentelemetry/#advanced-features","title":"Advanced Features","text":""},{"location":"opentelemetry/#span-attributes","title":"Span Attributes","text":"<pre><code>Tel.withSpan(\"processing\",\n  \"input.size\" -&gt; data.size,\n  \"batch.id\" -&gt; batchId\n) {\n  /* processing logic */\n}\n</code></pre>"},{"location":"opentelemetry/#nested-spans","title":"Nested Spans","text":"<p>Spans automatically nest when called within each other: <pre><code>Tel.withSpan(\"outer\") {\n  val result = Tel.withSpan(\"inner\") {\n    /* nested processing */\n    computeResult()\n  }\n  result\n}\n</code></pre></p>"},{"location":"opentelemetry/#no-op-by-default","title":"No-Op by Default","text":"<p>Without an <code>Etl4sTelemetry</code>, all calls are no-ops with zero overhead: <pre><code>/* No implicit provider - all Tel calls do nothing */\nTel.withSpan(\"processing\") { Tel.addCounter(\"processed\", 1) }\n</code></pre></p>"},{"location":"opentelemetry/#api-reference","title":"API Reference","text":""},{"location":"opentelemetry/#tel-object","title":"Tel Object","text":"Method Description <code>Tel.withSpan(name)(block)</code> Execute block in named span <code>Tel.addCounter(name, value)</code> Increment counter <code>Tel.setGauge(name, value)</code> Set gauge value <code>Tel.recordHistogram(name, value)</code> Record histogram value"},{"location":"opentelemetry/#etl4stelemetry-interface","title":"Etl4sTelemetry Interface","text":"Method Description <code>withSpan(name, attrs*)(block)</code> Create span around block <code>addCounter(name, value)</code> Record counter increment <code>setGauge(name, value)</code> Set gauge to value <code>recordHistogram(name, value)</code> Record histogram measurement"},{"location":"opentelemetry/#built-in-implementations","title":"Built-in Implementations","text":"Implementation Description <code>Etl4sConsoleTelemetry()</code> Prints to stdout <code>Etl4sNoOpTelemetry</code> Silent no-op (default)"},{"location":"operators/","title":"Operators","text":"<p>etl4s uses a few simple operators to build pipelines:</p> Operator Name Description Example <code>~&gt;</code> Connect Chains operations in sequence <code>e1 ~&gt; t1 ~&gt; l1</code> <code>&amp;</code> Combine Group sequential operations with same input <code>t1 &amp; t2</code> <code>&amp;&gt;</code> Parallel Group concurrent operations with same input <code>t1 &amp;&gt; t2</code> <code>&gt;&gt;</code> Sequence Runs pipelines or nodes in order (ignoring previous output) <code>p1 &gt;&gt; p2</code>"},{"location":"tasks/","title":"Parallel Execution","text":"<p>etl4s has an elegant shorthand for grouping and parallelizing operations that share the same input type: <pre><code>import etl4s._\n\n/* Simulate slow IO operations (e.g: DB calls, API requests) */\n\nval e1 = Extract { Thread.sleep(100); 42 }\nval e2 = Extract { Thread.sleep(100); \"hello\" }\nval e3 = Extract { Thread.sleep(100); true }\n</code></pre></p> <p>Sequential run of e1, e2, and e3 (~300ms total) <pre><code>val sequential: Extract[Unit, ((Int, String), Boolean)] =\n     e1 &amp; e2 &amp; e3\n</code></pre></p> <p>Parallel run of e1, e2, e3 on their own JVM threads with Scala Futures (~100ms total, same result, 3X faster) <pre><code>import scala.concurrent.ExecutionContext.Implicits.global\n\nval parallel: Extract[Unit, ((Int, String), Boolean)] =\n     e1 &amp;&gt; e2 &amp;&gt; e3\n</code></pre> Use the built-in zip method to flatten unwieldly nested tuples: <pre><code>val clean: Extract[Unit, (Int, String, Boolean)] =\n     (e1 &amp; e2 &amp; e3).zip\n</code></pre> Mix sequential and parallel execution (First two parallel (~100ms), then third (~100ms)): <pre><code>val mixed = (e1 &amp;&gt; e2) &amp; e3\n</code></pre></p> <p>Full example of a parallel pipeline: <pre><code>val consoleLoad: Load[String, Unit] = Load(println(_))\nval dbLoad:      Load[String, Unit] = Load(x =&gt; println(s\"DB Load: ${x}\"))\n\nval merge = Transform[(Int, String, Boolean), String] { t =&gt; \n    val (i, s, b) = t\n    s\"$i-$s-$b\"\n  }\n\nval pipeline =\n  (e1 &amp;&gt; e2 &amp;&gt; e3).zip ~&gt; merge ~&gt; (consoleLoad &amp;&gt; dbLoad)\n</code></pre></p>"},{"location":"testing/","title":"Testing","text":"<p>Use etl4s with the testing framework of your choice</p> <p>Run nodes like normal functions <pre><code>import etl4s._\n\nval times5: Transform[Int, Int] = Transform(_ * 5)\n\ntimes5(5)\n</code></pre></p> <p>You will get: <pre><code>25\n</code></pre></p> <p>Run pipelines with <code>unsafeRun</code> or <code>safeRun</code>:</p> <p><pre><code>import etl4s._\n\nval plus2:  Transform[Int, Int] = Transform(_ + 2)\nval times5: Transform[Int, Int] = Transform(_ * 5)\n\nval p: Pipeline[Int, Int] = plus2 ~&gt; times5\n\np.unsafeRun(2)\n</code></pre> Gives <pre><code>20\n</code></pre> However, if you use <code>safeRun</code> as below <pre><code>p.safeRun(2)\n</code></pre> You will get a response wrapped in a <code>scala.util.Try</code> <pre><code>Success(20)\n</code></pre></p>"},{"location":"testing/#testing-with-traces","title":"Testing with Traces","text":"<p>For testing with execution insights, see the Pipeline Tracing section. You can test traced execution and cross-node communication:</p> <pre><code>import etl4s._\n\nval pipeline = Transform[String, Int](_.length)\nval trace = pipeline.unsafeRunTrace(\"test\")\n\nassert(trace.result == 4)\nassert(trace.timeElapsedMillis &gt;= 0)\nassert(!trace.hasErrors)\n</code></pre>"},{"location":"trace/","title":"Runtime State","text":"<p>Nodes can access and update their runtime state:</p> <ul> <li>Access current execution state (<code>Trace.current</code>, <code>Trace.hasErrors</code>, etc)</li> <li>Update runtime state (<code>Trace.log()</code>, <code>Trace.error()</code>). Thread-safe and append only</li> <li>Share state automatically across the entire pipeline. All your Nodes \"know\" what happened before them and can act with this knowledge</li> </ul> <p>Use <code>runTrace</code> to get all your logs and errors cleanly after you've run your pipeline.</p> <p>How it works: Trace uses two ThreadLocal channels (like Unix stdout/stderr) that automatically accumulate across your pipeline - thread-safe with minimal overhead:</p> <pre><code>val A = Transform[String, Int] { s =&gt;\n  Trace.log(\"Processing\")\n  s.length\n}\n\nval res: Int = A.unsafeRun(\"hello\")  // 5\nval trace: Trace[Int] = A.unsafeRunTrace(\"hello\")\n</code></pre> <pre><code>Trace(\n  result = 5,\n  logs = List(\"Processing\"),\n  errors = List(),\n  timeElapsedMillis = 2L\n)\n</code></pre>"},{"location":"trace/#nodes-that-react-to-each-other","title":"Nodes That React to Each Other","text":"<p>Downstream nodes can instantly see what happened upstream and adapt their behavior.</p> <pre><code>val A = Transform[String, Int] { s =&gt;\n  if (s.isEmpty) Trace.error(\"empty\")\n  s.length\n}\n\nval B = Transform[Int, String] { n =&gt;\n  if (Trace.hasErrors) \"FALLBACK\" else s\"len: $n\"  \n}\n\nval pipeline = A ~&gt; B\n\npipeline.unsafeRun(\"hello\")  /* \"len: 5\" */\npipeline.unsafeRun(\"\")       /* \"FALLBACK\" */\n</code></pre> <p>No wiring required. The downstream node automatically knows about upstream problems and switches to fallback mode since it can access the run's <code>Trace</code></p>"},{"location":"trace/#debug-any-pipeline-instantly","title":"Debug Any Pipeline Instantly","text":"<pre><code>val p = Transform[String, Int] { input =&gt;\n  Trace.log(\"Processing started\")\n  if (input.isEmpty) Trace.error(\"Empty input!\")\n  input.length * 2\n}\n\nval trace = p.unsafeRunTrace(\"test\")\n</code></pre> <p>Get everything in one shot: <pre><code>Trace(\n  result = 8,\n  logs = List(\"Processing started\"),\n  errors = List(),\n  timeElapsedMillis = 2L\n)\n</code></pre></p>"},{"location":"trace/#live-pipeline-state","title":"Live Pipeline State","text":"<p>In any <code>Node</code> you can check what is happening right now with <code>Trace.getCurrent</code></p> <pre><code>val p = Transform[String, String] { input =&gt;\n  val current = Trace.getCurrent\n  if (current.timeElapsedMillis &gt; 1000) {\n    \"TIMEOUT\"  /* Fast path for slow executions */\n  } else {\n    input.toUpperCase\n  }\n}\n</code></pre> <p>Or use the direct getters: <pre><code>val p = Transform[String, String] { input =&gt;\n  if (Trace.getElapsedTimeMillis &gt; 1000) {\n    \"TIMEOUT\"  /* Fast path for slow executions */\n  } else {\n    input.toUpperCase\n  }\n}\n</code></pre></p> <p>React to problems instantly: <pre><code>if (Trace.hasErrors) {\n  /* Switch to fallback mode */\n} else {\n  /* Continue normal processing */\n}\n</code></pre></p>"},{"location":"trace/#quick-reference","title":"Quick Reference","text":""},{"location":"trace/#logging-and-error-reporting","title":"Logging and Error Reporting","text":"Method Description Example <code>Trace.log(message)</code> Log any value <code>Trace.log(\"Processing started\")</code> <code>Trace.error(err)</code> Log error <code>Trace.error(\"Invalid format\")</code>"},{"location":"trace/#state-checking","title":"State Checking","text":"Method Description Example <code>Trace.hasErrors</code> Check for errors <code>if (Trace.hasErrors) ...</code> <code>Trace.hasLogs</code> Check for logs <code>if (Trace.hasLogs) ...</code>"},{"location":"trace/#getting-current-state","title":"Getting Current State","text":"Method Description Example <code>Trace.getCurrent</code> Get live execution state <code>val state = Trace.getCurrent</code> <code>Trace.getLogs</code> Current logs <code>val logs = Trace.getLogs</code> <code>Trace.getErrors</code> Current errors <code>val errors = Trace.getErrors</code> <code>Trace.getLogsAsStrings</code> Logs as strings <code>val logStrings = Trace.getLogsAsStrings</code> <code>Trace.getErrorsAsStrings</code> Errors as strings <code>val errorStrings = Trace.getErrorsAsStrings</code>"},{"location":"trace/#timing-information","title":"Timing Information","text":"Method Description Example <code>Trace.getElapsedTimeMillis</code> Time in milliseconds <code>val ms = Trace.getElapsedTimeMillis</code> <code>Trace.getElapsedTimeSeconds</code> Time in seconds <code>val secs = Trace.getElapsedTimeSeconds</code>"},{"location":"trace/#counts-and-recent-items","title":"Counts and Recent Items","text":"Method Description Example <code>Trace.getLogCount</code> Number of logs <code>val count = Trace.getLogCount</code> <code>Trace.getErrorCount</code> Number of errors <code>val count = Trace.getErrorCount</code> <code>Trace.getLastLog</code> Most recent log <code>val last = Trace.getLastLog</code> <code>Trace.getLastError</code> Most recent error <code>val last = Trace.getLastError</code>"},{"location":"trace/#trace-result-properties","title":"Trace Result Properties","text":"Property Type Description <code>result</code> <code>A</code> or <code>Try[A]</code> Execution result <code>logs</code> <code>List[Any]</code> Collected log values <code>timeElapsedMillis</code> <code>Long</code> Execution time in ms <code>errors</code> <code>List[Any]</code> Errors <code>hasErrors</code> <code>Boolean</code> Quick error check <code>seconds</code> <code>Double</code> Timing in seconds <p>This makes etl4s pipelines fully observable and self-aware - nodes can communicate, react to problems, and provide rich debugging information automatically.</p>"},{"location":"validation/","title":"Validation","text":"<p>etl4s provides a lightweight validation system that lets you accumulate errors instead of failing at the first problem. </p> Component Description Example <code>Validated[T]</code> Type class for validating objects <code>Validated[User] validator</code> <code>ValidationResult</code> Success (Valid) or failure (Invalid) <code>Valid(user)</code> or <code>Invalid(errors)</code> <code>require</code> Validate a condition <code>require(user, user.age &gt;= 18, \"Minor\")</code> <code>success</code> Create successful validation <code>success(user)</code> <code>failure</code> Create failed validation <code>failure(\"Invalid data\")</code> <code>&amp;&amp;</code> Combine with AND logic <code>validateName &amp;&amp; validateEmail</code> <code>||</code> operator Combine with OR logic <code>isPremium || isAdmin</code>"},{"location":"validation/#core-usage","title":"Core Usage","text":"<p>Create a simple validator <pre><code>import etl4s._\n\ncase class User(name: String, email: String, age: Int)\n\nval validateUser = Validated[User] { user =&gt;\n  require(user, user.name.nonEmpty, \"Name required\") &amp;&amp;\n  require(user, user.email.contains(\"@\"), \"Invalid email\") &amp;&amp;\n  require(user, user.age &gt;= 18, \"Must be 18+\")\n}\n\nvalidateUser(User(\"Alice\", \"a@mail.com\", 25))  // \u2705 Valid\nvalidateUser(User(\"\", \"bad\", 15))              // \u274c Invalid(List(...))\n</code></pre></p>"},{"location":"validation/#build-modular-validators","title":"Build Modular Validators","text":"<p>Compose validators with <code>&amp;&amp;</code> and <code>||</code> <pre><code>val nameCheck  = Validated[User](u =&gt; require(u, u.name.nonEmpty, \"Name required\"))\nval emailCheck = Validated[User](u =&gt; require(u, u.email.contains(\"@\"), \"Email bad\"))\nval ageCheck   = Validated[User](u =&gt; require(u, u.age &gt;= 18, \"Must be 18+\"))\n\nval basic   = nameCheck &amp;&amp; emailCheck\nval fallback = nameCheck || ageCheck\n</code></pre></p>"},{"location":"validation/#conditional-validation","title":"Conditional Validation","text":"<p>Write validation flows with conditional branching <pre><code>val specialValidator = Validated[User] { user =&gt;\n  val base = require(user, user.name.nonEmpty, \"Name required\")\n\n  if (user.name == \"Admin\")\n    base &amp;&amp; require(user, user.age &gt;= 21, \"Admins must be 21+\")\n  else if (user.email.endsWith(\".gov\"))\n    base &amp;&amp; success(user)\n  else\n    base &amp;&amp; require(user, user.age &gt;= 18, \"Must be 18+\")\n}\n</code></pre></p>"},{"location":"validation/#real-pipeline-use","title":"Real pipeline use","text":"<pre><code>val extract = Extract[Unit, List[User]](_ =&gt; List(\n  User(\"Alice\", \"alice@mail.com\", 25),\n  User(\"\", \"bad@\", 15)\n))\n\nval split = Transform[List[User], (List[User], List[User])] { users =&gt;\n  users.partition(validateUser(_).isValid)\n}\n\nval summarize = Transform[(List[User], List[User]), String] {\n  case (ok, bad) =&gt; s\"${ok.size} valid / ${bad.size} invalid\"\n}\n\nval pipeline = extract ~&gt; split ~&gt; summarize\npipeline.unsafeRun(())  /* \"1 valid / 1 invalid\" */\n</code></pre>"}]}